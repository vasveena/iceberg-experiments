{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30470b8e",
   "metadata": {},
   "source": [
    "# Spark-Iceberg Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac6dba5",
   "metadata": {},
   "source": [
    "# Index\n",
    "\n",
    "1) [Configuring Iceberg](#configure_iceberg) <br>\n",
    "2) [Evaluating Iceberg for Spark](#test) <br>\n",
    "&emsp;&emsp;&emsp;&emsp;a) [Regular Dataframe behavior](#normaldftest) <br>\n",
    "&emsp;&emsp;&emsp;&emsp;b) [Pure Dataframe + Iceberg](#dftest) <br>\n",
    "&emsp;&emsp;&emsp;&emsp;c) [SparkSQL + Iceberg](#sparksqltest) <br>\n",
    "3) [Hybrid tests with Dataframe and Datasets](#hybrid) <br>\n",
    "&emsp;&emsp;&emsp;&emsp;a) [SparkSQL + Iceberg + Dataframe](#sparksqldftest) <br>\n",
    "&emsp;&emsp;&emsp;&emsp;b) [SparkSQL + Iceberg + Dataset](#sparksqldstest) <br>\n",
    "4) [Perform transformations with UDFs and Dataset APIs](#udf) <br>\n",
    "5) [Known Issues](#knownissues) <br>\n",
    "6) [Key Takeaways](#summary) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b62ad1",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"configure_iceberg\"></a>\n",
    "# Configuring Iceberg on Spark session\n",
    "\n",
    "This notebook uses 3 * r4.4xlarge EMR 6.2 cluster. Iceberg 0.11 only works with Spark 3.0.1 due to this issue: https://github.com/apache/iceberg/issues/2335. Iceberg 0.12, yet to be released, will support Spark 3.1 on EMR 6.3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "435ac1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>16</td><td>application_1619633879001_0026</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-41-141.ec2.internal:20888/proxy/application_1619633879001_0026/\" >Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-44-57.ec2.internal:8042/node/containerlogs/container_1619633879001_0026_01_000001/livy\" >Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars.packages': 'org.apache.iceberg:iceberg-spark3-runtime:0.11.0', 'spark.sql.extensions': 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions', 'spark.sql.catalog.spark_catalog': 'org.apache.iceberg.spark.SparkSessionCatalog', 'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.sql.catalog.local': 'org.apache.iceberg.spark.SparkCatalog', 'spark.sql.catalog.local.type': 'hadoop', 'spark.sql.catalog.local.warehouse': 's3://vasveena-test-demo/iceberg/catalog/tables/'}, 'proxyUser': 'user_vasveena', 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>16</td><td>application_1619633879001_0026</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-41-141.ec2.internal:20888/proxy/application_1619633879001_0026/\" >Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-44-57.ec2.internal:8042/node/containerlogs/container_1619633879001_0026_01_000001/livy\" >Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":  { \n",
    "             \"spark.jars.packages\":\"org.apache.iceberg:iceberg-spark3-runtime:0.11.0\",\n",
    "             \"spark.sql.extensions\":\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "             \"spark.sql.catalog.spark_catalog\":\"org.apache.iceberg.spark.SparkSessionCatalog\",\n",
    "             \"spark.sql.catalog.spark_catalog.type\":\"hive\",\n",
    "             \"spark.sql.catalog.local\":\"org.apache.iceberg.spark.SparkCatalog\",\n",
    "             \"spark.sql.catalog.local.type\":\"hadoop\",\n",
    "             \"spark.sql.catalog.local.warehouse\":\"s3://vasveena-test-demo/iceberg/catalog/tables/\"\n",
    "           } \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cfb0fe8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23805a0e6d44697b9b13629ff21a61c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res1: String = 3.0.1-amzn-0\n"
     ]
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb6af0e",
   "metadata": {},
   "source": [
    "<a id=\"test\"></a>\n",
    "# Evaluating Iceberg for Spark\n",
    "<a id=\"normaldftest\"></a>\n",
    "## Regular Dataframe Read/Write test with Parquet Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b309df4",
   "metadata": {},
   "source": [
    "### Read Input data from S3\n",
    "\n",
    "Let's read our 100M record dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fe1ebdfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6145a967667e42c688d6f0010b3e4514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_df: org.apache.spark.sql.DataFrame = [id: bigint, month: bigint ... 5 more fields]\n",
      "res9: Long = 100000000\n"
     ]
    }
   ],
   "source": [
    "val input_df = spark.read.parquet(\"s3://neilawstmp2/tmp/hudi-perf/input/\")\n",
    "input_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d0e0dc9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf97fd192dd466ca5d01cb6a40fcbbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- month: long (nullable = true)\n",
      " |-- sk: long (nullable = true)\n",
      " |-- txt: struct (nullable = true)\n",
      " |    |-- key1: string (nullable = true)\n",
      " |-- uuid: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- modified_timestamp: timestamp (nullable = true)\n",
      "\n",
      "res12: Int = 25\n"
     ]
    }
   ],
   "source": [
    "input_df.printSchema()\n",
    "input_df.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c01767",
   "metadata": {},
   "source": [
    "### Transform input data\n",
    "Performing some transformations on the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fc55a678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa577d5aa8e4423bc76db63503a58e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.functions._\n",
      "input_df2: org.apache.spark.sql.DataFrame = [id: bigint, month: bigint ... 9 more fields]\n",
      "+-------+-----+-------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|     id|month|     sk|txt|                uuid|year| modified_timestamp|  z|schema-v|data-v|  trade_dt|\n",
      "+-------+-----+-------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|4000000|    3|4000000|[E]|6e505939-f5fd-4ab...|2019|2021-04-02 00:05:02|  9|      v1|    v2|2021-04-02|\n",
      "|4000001|    9|4000001|[F]|20486aca-2759-43f...|2019|2021-04-02 00:05:02|  d|      v1|    v2|2021-04-02|\n",
      "|4000002|   11|4000002|[G]|42962a21-a2dc-40d...|2019|2021-04-02 00:05:02|  d|      v1|    v2|2021-04-02|\n",
      "|4000003|    9|4000003|[H]|9841ad6d-1532-496...|2019|2021-04-02 00:05:02|  c|      v1|    v2|2021-04-02|\n",
      "|4000004|    4|4000004|[I]|ff1a855a-cced-495...|2019|2021-04-02 00:05:02|  4|      v1|    v2|2021-04-02|\n",
      "+-------+-----+-------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val input_df2=(input_df.withColumn(\"z\", substring(md5(concat($\"id\")),1,1))\n",
    "                       .withColumn(\"schema-v\", lit(\"v1\")).withColumn(\"data-v\", lit(\"v2\"))\n",
    "                       .withColumn(\"trade_dt\", substring($\"modified_timestamp\",1,10)))\n",
    "input_df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a2901f",
   "metadata": {},
   "source": [
    "### Write data back to S3 in normal Parquet format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4324b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "187aaa4589584fbb88d230f5af7b48c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: Long = 1649793133537949\n",
      "s3_location: String = s3://vasveena-test-demo/temp6/parquet-perf/catalog/example_iceberg_perf_test_6\n",
      "duration: String = 97.146739323 seconds\n"
     ]
    }
   ],
   "source": [
    "val t1 = System.nanoTime\n",
    "\n",
    "val s3_location=s\"s3://vasveena-test-demo/temp6/parquet-perf/catalog/example_iceberg_perf_test_6\"\n",
    "\n",
    "input_df2.write.mode(\"OVERWRITE\").partitionBy(\"z\",\"schema-v\",\"data-v\",\"trade_dt\").parquet(s3_location)\n",
    "\n",
    "val duration = (System.nanoTime - t1) / 1e9d + \" seconds\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737e35f4",
   "metadata": {},
   "source": [
    "### Write data back to S3 in Spark table format (Parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80442d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15b40a63cb0f49479bac86ff1ae2632b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: Long = 1650088339766019\n",
      "s3_location_ndf: String = s3://vasveena-test-demo/iceberg/tables/catalog/example_iceberg_perf_test_normaldf\n",
      "duration: String = 100.121327602seconds\n"
     ]
    }
   ],
   "source": [
    "val t1 = System.nanoTime\n",
    "\n",
    "val s3_location_ndf=s\"s3://vasveena-test-demo/iceberg/tables/catalog/example_iceberg_perf_test_normaldf\"\n",
    "\n",
    "(input_df2.write.mode(\"OVERWRITE\")\n",
    "    .option(\"path\", s3_location_ndf)\n",
    "    .partitionBy(\"z\",\"schema-v\",\"data-v\",\"trade_dt\").format(\"parquet\")\n",
    "    .saveAsTable(\"iceberg_table_normaldf\"))\n",
    "\n",
    "val duration = (System.nanoTime - t1) / 1e9d + \"seconds\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34c24fe",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Reading output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "805348b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3183d11d8b084ba99d015c535e821fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading direct Parquet output\n",
      "+-------+-----+-------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|     id|month|     sk|txt|                uuid|year| modified_timestamp|  z|schema-v|data-v|  trade_dt|\n",
      "+-------+-----+-------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|4000011|    2|4000011|[P]|dbf28c6d-6f94-449...|2019|2021-04-02 00:05:02|  3|      v1|    v2|2021-04-02|\n",
      "|4000017|    1|4000017|[V]|96fa033f-0fb0-4d5...|2019|2021-04-02 00:05:02|  3|      v1|    v2|2021-04-02|\n",
      "|4000047|   10|4000047|[Z]|93c488d8-b882-442...|2019|2021-04-02 00:05:02|  3|      v1|    v2|2021-04-02|\n",
      "|4000051|    2|4000051|[D]|4464b11b-14aa-4ca...|2019|2021-04-02 00:05:02|  3|      v1|    v2|2021-04-02|\n",
      "|4000071|    4|4000071|[X]|bdb89389-2406-47b...|2019|2021-04-02 00:05:02|  3|      v1|    v2|2021-04-02|\n",
      "+-------+-----+-------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Reading Spark table Parquet output\n",
      "+-------+-----+-------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|     id|month|     sk|txt|                uuid|year| modified_timestamp|  z|schema-v|data-v|  trade_dt|\n",
      "+-------+-----+-------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|1000015|   12|1000015|[D]|4830bd4b-08ee-4ce...|2019|2021-04-02 00:05:02|  b|      v1|    v2|2021-04-02|\n",
      "|1000046|    1|1000046|[I]|86f33c51-b599-455...|2019|2021-04-02 00:05:02|  b|      v1|    v2|2021-04-02|\n",
      "|1000082|    2|1000082|[S]|fcc246fa-bf90-411...|2019|2021-04-02 00:05:02|  b|      v1|    v2|2021-04-02|\n",
      "|1000084|    5|1000084|[U]|c5a1da16-e7a5-484...|2019|2021-04-02 00:05:02|  b|      v1|    v2|2021-04-02|\n",
      "|1000095|    8|1000095|[F]|284cb5a7-3fa3-43f...|2019|2021-04-02 00:05:02|  b|      v1|    v2|2021-04-02|\n",
      "+-------+-----+-------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"Reading direct Parquet output\")\n",
    "spark.read.parquet(\"s3://vasveena-test-demo/temp6/parquet-perf/catalog/example_iceberg_perf_test_6/\").show(5)\n",
    "println(\"Reading Spark table Parquet output\")\n",
    "spark.sql(\"select * from iceberg_table_normaldf limit 5\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d73713",
   "metadata": {},
   "source": [
    "Now lets go ahead and list the S3 location where our DF was written in regular Parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af468f6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE z=0/\n",
      "                           PRE z=1/\n",
      "                           PRE z=2/\n",
      "                           PRE z=3/\n",
      "                           PRE z=4/\n",
      "                           PRE z=5/\n",
      "                           PRE z=6/\n",
      "                           PRE z=7/\n",
      "                           PRE z=8/\n",
      "                           PRE z=9/\n",
      "                           PRE z=a/\n",
      "                           PRE z=b/\n",
      "                           PRE z=c/\n",
      "                           PRE z=d/\n",
      "                           PRE z=e/\n",
      "                           PRE z=f/\n",
      "2021-05-17 20:28:35          0 _SUCCESS\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "aws s3 ls s3://vasveena-test-demo/temp6/parquet-perf/catalog/example_iceberg_perf_test_6/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8f1e29",
   "metadata": {},
   "source": [
    "Checking the S3 location where our DF was written in regular Spark table format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd345568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE z=0/\n",
      "                           PRE z=1/\n",
      "                           PRE z=2/\n",
      "                           PRE z=3/\n",
      "                           PRE z=4/\n",
      "                           PRE z=5/\n",
      "                           PRE z=6/\n",
      "                           PRE z=7/\n",
      "                           PRE z=8/\n",
      "                           PRE z=9/\n",
      "                           PRE z=a/\n",
      "                           PRE z=b/\n",
      "                           PRE z=c/\n",
      "                           PRE z=d/\n",
      "                           PRE z=e/\n",
      "                           PRE z=f/\n",
      "2021-05-17 20:33:28          0 _SUCCESS\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "aws s3 ls s3://vasveena-test-demo/iceberg/tables/catalog/example_iceberg_perf_test_normaldf/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fad8355",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "As expected, the S3 partitions are not hashed in both Spark table format and direct Parquet writes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32df9b4",
   "metadata": {},
   "source": [
    "<a id=\"dftest\"></a>\n",
    "## Dataframe Read/Write tests with Iceberg format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00943c1d",
   "metadata": {},
   "source": [
    "### Bulk insert our input dataframe in Iceberg format\n",
    "\n",
    "Now lets try to write the transformed dataframe in Iceberg format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f1a81f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "248176a444e942b4b1e00a94f3c2c020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: Long = 1650425623897933\n",
      "s3_location_idf: String = s3://vasveena-test-demo/iceberg/tables/catalog/example_iceberg_perf_test_dfonly\n",
      "data_location_idf: String = s3://vasveena-test-hmswarehouse/\n",
      "duration: String = 171.271756516seconds\n"
     ]
    }
   ],
   "source": [
    "val t1 = System.nanoTime\n",
    "\n",
    "val s3_location_idf=s\"s3://vasveena-test-demo/iceberg/tables/catalog/example_iceberg_perf_test_dfonly\"\n",
    "val data_location_idf=s\"s3://vasveena-test-hmswarehouse/\"\n",
    "\n",
    "//orderBy clause is required to avoid error \"java.lang.IllegalStateException: Already closed files for partition: z=6/schema_v=v1/data_v=v2/trade_dt=trade_dt\"\n",
    "(input_df2.orderBy(\"z\",\"`schema-v`\",\"`data-v`\",\"trade_dt\").write.mode(\"OVERWRITE\")\n",
    "    .option(\"path\", s3_location_idf)\n",
    "    .option(\"write.object-storage.enabled\",true)\n",
    "    .option(\"write.object-storage.path\",data_location_idf)\n",
    "    .partitionBy(\"z\",\"`schema-v`\",\"`data-v`\",\"trade_dt\").format(\"iceberg\")\n",
    "    .saveAsTable(\"iceberg_table_dfonly22\"))\n",
    "\n",
    "val duration = (System.nanoTime - t1) / 1e9d + \"seconds\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31ee690",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Reading output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f67e304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f93670a6fc3c412499deefceca5565a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- month: long (nullable = true)\n",
      " |-- sk: long (nullable = true)\n",
      " |-- txt: struct (nullable = true)\n",
      " |    |-- key1: string (nullable = true)\n",
      " |-- uuid: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- modified_timestamp: timestamp (nullable = true)\n",
      " |-- z: string (nullable = true)\n",
      " |-- schema-v: string (nullable = true)\n",
      " |-- data-v: string (nullable = true)\n",
      " |-- trade_dt: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"iceberg\").load(\"iceberg_table_dfonly22\").printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d09dfcba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e39b76a11f114dc3993fcb6553bd145b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data in iceberg format\n",
      "+--------+-----+--------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|      id|month|      sk|txt|                uuid|year| modified_timestamp|  z|schema-v|data-v|  trade_dt|\n",
      "+--------+-----+--------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|44000005|    6|44000005|[X]|879bf2e9-92f1-44d...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|44000023|    7|44000023|[P]|141ee5fc-0222-40e...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|44000070|    6|44000070|[K]|fd1b1435-78a6-4f0...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|44000091|    2|44000091|[F]|0ccdc268-8dd9-4ba...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|44000099|    8|44000099|[N]|2271f8df-4eff-445...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "+--------+-----+--------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Using SparkSQL\n",
      "+--------+-----+--------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|      id|month|      sk|txt|                uuid|year| modified_timestamp|  z|schema-v|data-v|  trade_dt|\n",
      "+--------+-----+--------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|44000005|    6|44000005|[X]|879bf2e9-92f1-44d...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|44000023|    7|44000023|[P]|141ee5fc-0222-40e...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|44000070|    6|44000070|[K]|fd1b1435-78a6-4f0...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|44000091|    2|44000091|[F]|0ccdc268-8dd9-4ba...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|44000099|    8|44000099|[N]|2271f8df-4eff-445...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "+--------+-----+--------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"Reading data in iceberg format\")\n",
    "spark.read.format(\"iceberg\").load(\"iceberg_table_dfonly22\").show(5)\n",
    "println(\"Using SparkSQL\")\n",
    "spark.sql(\"select * from iceberg_table_dfonly22 limit 5\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eda8a7a",
   "metadata": {},
   "source": [
    "Listing S3 location of Iceberg data and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b049bddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE data/\n",
      "                           PRE metadata/\n",
      "2021-05-17 20:40:19       3604 00000-a606ccd1-7242-4963-81a0-19f85b7997a9.metadata.json\n",
      "2021-05-17 20:40:18       9457 53e4ba81-c785-4b3b-8a08-4de084692421-m0.avro\n",
      "2021-05-17 20:40:19       3565 snap-7125006042262973350-1-53e4ba81-c785-4b3b-8a08-4de084692421.avro\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "#Metastore location is empty. i.e, no pre-hash\n",
    "aws s3 ls \"s3://vasveena-test-hmswarehouse/\"\n",
    "\n",
    "#Listing table location\n",
    "aws s3 ls \"s3://vasveena-test-demo/iceberg/tables/catalog/example_iceberg_perf_test_dfonly/\"\n",
    "\n",
    "#Lists iceberg metadata\n",
    "aws s3 ls \"s3://vasveena-test-demo/iceberg/tables/catalog/example_iceberg_perf_test_dfonly/metadata/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6d421c",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "S3 partitions are not hashed when using pure dataframe with iceberg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f1fb76",
   "metadata": {},
   "source": [
    "<a id=\"sparksqltest\"></a>\n",
    "## SparkSQL + Iceberg Read/Write test with Parquet Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1bc9a5",
   "metadata": {},
   "source": [
    "### Creating an output table in SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "16aa02bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "430af1e8c4b14cbe8d0d0e7fc00c7ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res7: org.apache.spark.sql.DataFrame = []\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE iceberg_table_sparksql22 (id bigint,\n",
    "                                       month bigint,\n",
    "                                       sk bigint,\n",
    "                                       txt struct<key1:string>,\n",
    "                                       uuid string,\n",
    "                                       year string,\n",
    "                                       modified_timestamp timestamp,\n",
    "                                       z string,\n",
    "                                       `schema_v` string,   --had to rename these columns due to error\n",
    "                                       `data_v` string,\n",
    "                                       trade_dt string)\n",
    "USING iceberg\n",
    "OPTIONS ( 'write.object-storage.enabled'=true,\n",
    "          'write.object-storage.path'='s3://vasveena-test-hmswarehouse/')\n",
    "PARTITIONED BY (z,`schema_v`,`data_v`,trade_dt)\n",
    "LOCATION 's3://vasveena-test-demo/iceberg/tables/catalog/example_iceberg_perf_test_sparksql22'\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5220cc54",
   "metadata": {},
   "source": [
    "During insert with input_df2, we will get an error \"Possibly unquoted identifier schema-v detected\" which happened even if those fields are quoted properly on both source and destination. So, changing col name from data-v, schema-v to data_v and schema_v on both source and destination tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5dab484d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fe643abab6840e68f3be32c560e646f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_df4: org.apache.spark.sql.DataFrame = [id: bigint, month: bigint ... 9 more fields]\n"
     ]
    }
   ],
   "source": [
    "val input_df4 = input_df2.withColumnRenamed(\"schema-v\", \"schema_v\").withColumnRenamed(\"data-v\", \"data_v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b00d0f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca661ab1b70743329a289a21da77a291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- month: long (nullable = true)\n",
      " |-- sk: long (nullable = true)\n",
      " |-- txt: struct (nullable = true)\n",
      " |    |-- key1: string (nullable = true)\n",
      " |-- uuid: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- modified_timestamp: timestamp (nullable = true)\n",
      " |-- z: string (nullable = true)\n",
      " |-- schema_v: string (nullable = false)\n",
      " |-- data_v: string (nullable = false)\n",
      " |-- trade_dt: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_df4.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8ed29e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb46f1fda79849c092d685cafe6d3c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: there was one deprecation warning (since 2.0.0); for details, enable `:setting -deprecation' or `:replay -deprecation'\n"
     ]
    }
   ],
   "source": [
    "input_df4.registerTempTable(\"inputdf4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ab375003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75c7d90dc87c48a2ad8dca77e00f834d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|      id|month|      sk|txt|                uuid|year| modified_timestamp|  z|schema_v|data_v|  trade_dt|\n",
      "+--------+-----+--------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|15000000|    7|15000000|[C]|d413c3ff-031f-490...|2019|2021-04-02 00:05:02|  e|      v1|    v2|2021-04-02|\n",
      "|15000001|    1|15000001|[D]|56137d34-8d22-46e...|2019|2021-04-02 00:05:02|  3|      v1|    v2|2021-04-02|\n",
      "|15000002|   11|15000002|[E]|042a812f-1009-403...|2019|2021-04-02 00:05:02|  c|      v1|    v2|2021-04-02|\n",
      "|15000003|    1|15000003|[F]|9a3d5aad-8d18-4d8...|2019|2021-04-02 00:05:02|  5|      v1|    v2|2021-04-02|\n",
      "|15000004|   12|15000004|[G]|0c8708bb-558f-4fd...|2019|2021-04-02 00:05:02|  f|      v1|    v2|2021-04-02|\n",
      "+--------+-----+--------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select * from inputdf4 limit 5\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd82e322",
   "metadata": {},
   "source": [
    "### Bulk insert data with iceberg format\n",
    "\n",
    "Now, lets do a bulk insert on iceberg table using pure SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9dc07b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3610dbe7ec0944a38e93739ce3b26f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: Long = 1663700435514845\n",
      "res25: org.apache.spark.sql.DataFrame = []\n",
      "duration: String = 136.060916253seconds\n"
     ]
    }
   ],
   "source": [
    "val t1 = System.nanoTime\n",
    "\n",
    "spark.sql(\"\"\"insert overwrite table iceberg_table_sparksql22\n",
    "             partition (z,`schema_v`,`data_v`,trade_dt) \n",
    "             select id, month, sk, txt, uuid,\n",
    "             year, modified_timestamp, z,\n",
    "             `schema_v`, `data_v`,trade_dt\n",
    "             from inputdf4\n",
    "             order by z,`schema_v`,`data_v`,trade_dt \"\"\") //order by clause is required to avoid error \"java.lang.IllegalStateException: Already closed files for partition: z=6/schema_v=v1/data_v=v2/trade_dt=trade_dt\"\n",
    "\n",
    "val duration = (System.nanoTime - t1) / 1e9d + \"seconds\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb58417",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Reading output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ddbc4654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95870d27cc3342c9b684019a764a20c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|     id|month|     sk|txt|                uuid|year| modified_timestamp|  z|schema_v|data_v|  trade_dt|\n",
      "+-------+-----+-------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|7000019|    7|7000019|[N]|b74241b1-ff28-4d6...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|7000047|    5|7000047|[P]|c0ffb8e0-f38c-45f...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|7000050|    3|7000050|[S]|446d3ea6-1d15-467...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|7000059|    3|7000059|[B]|9b9f4dd5-6cf7-4a3...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|7000069|    2|7000069|[L]|563a3c50-bac2-40d...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "+-------+-----+-------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select * from default.iceberg_table_sparksql22\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1083764",
   "metadata": {},
   "source": [
    "Listing write object storage path. As we can see, the hashes are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cbe93450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE 0196c656/\n",
      "                           PRE 058e9a6a/\n",
      "                           PRE 0a8968ba/\n",
      "                           PRE 0aea53a0/\n",
      "                           PRE 0f459a4a/\n",
      "                           PRE 11aa143d/\n",
      "                           PRE 196efb11/\n",
      "                           PRE 1aefec80/\n",
      "                           PRE 1bfd75f9/\n",
      "                           PRE 2022376d/\n",
      "                           PRE 233ddc65/\n",
      "                           PRE 25e13848/\n",
      "                           PRE 27c516c7/\n",
      "                           PRE 2ee645fd/\n",
      "                           PRE 3345d8c1/\n",
      "                           PRE 3686753d/\n",
      "                           PRE 3b8b1575/\n",
      "                           PRE 3f1efbc8/\n",
      "                           PRE 4783b0e6/\n",
      "                           PRE 488a6052/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "aws s3 ls \"s3://vasveena-test-hmswarehouse/\" | head -n 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95e2e18",
   "metadata": {},
   "source": [
    "If we list one of the above hashes, we get the following:\n",
    "\n",
    "```\n",
    "aws s3 ls s3://vasveena-test-hmswarehouse/7e5ce3b1/catalog/example_iceberg_perf_test_sparksql22/z=2/schema_v=v1/data_v=v2/trade_dt=2021-04-02/\n",
    "2021-05-17 20:20:58  163809081 00002-213-ebfd0f70-dcd9-4359-ad57-5a54446e87c0-00001.parquet\n",
    "\n",
    "```\n",
    "\n",
    "Similarly, we can list the S3 metadata location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "92248a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-17 23:55:53       2917 00000-d8bff426-01b5-4469-b30b-ac91083ecc3f.metadata.json\n",
      "2021-05-18 00:09:52       3946 00001-34c06a63-a841-43b9-a59c-c6b206a2b575.metadata.json\n",
      "2021-05-18 00:21:01       5122 00002-6f56828d-3d39-4a9b-abf7-e71f4d4f1fad.metadata.json\n",
      "2021-05-18 00:09:52       9523 5588c135-8b04-4425-8747-65f431a15cb1-m0.avro\n",
      "2021-05-18 00:21:00       9521 c520445a-6898-47a4-a846-be7b6b0d8836-m0.avro\n",
      "2021-05-18 00:21:01       9484 c520445a-6898-47a4-a846-be7b6b0d8836-m1.avro\n",
      "2021-05-18 00:21:01       3615 snap-169749560302296372-1-c520445a-6898-47a4-a846-be7b6b0d8836.avro\n",
      "2021-05-18 00:09:52       3568 snap-5010499931560731436-1-5588c135-8b04-4425-8747-65f431a15cb1.avro\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "aws s3 ls \"s3://vasveena-test-demo/iceberg/tables/catalog/example_iceberg_perf_test_sparksql22/metadata/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d45cbc5",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "Spark + Iceberg integration with write object storage path works as documented while using pure SparkSQL. This is true for both Scala/PySpark implementations\n",
    "\n",
    "Table created via DF API and SparkSQL look identical based on diffchecker. Checked \"show create table\" from Hive catalog. This is something that needs to be checked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2aa555",
   "metadata": {},
   "source": [
    "<a id=\"hybrid\"></a>\n",
    "# Hybrid implementation with DF + DS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15ad673",
   "metadata": {},
   "source": [
    "Now that we have established pure SparkSQL works as expected, we will look into extending this implementation to DF + DS. The reason is, FINRA uses Datasets currently to read/write. \n",
    "\n",
    "FINRA's current workloads look like following\n",
    "\n",
    "&emsp;&emsp;&emsp;&emsp;1) Read BZ2/Parquet input files from S3 into dataset <br>\n",
    "&emsp;&emsp;&emsp;&emsp;2) Perform transformations on input dataset <br>\n",
    "&emsp;&emsp;&emsp;&emsp;3) Store intermediate results in HDFS<br>\n",
    "&emsp;&emsp;&emsp;&emsp;4) Do 2 and 3 for all N steps <br>\n",
    "&emsp;&emsp;&emsp;&emsp;5) Combine all HDFS output and write to S3 location (final destination)<br>\n",
    "\n",
    "Idea is to do something like following\n",
    "\n",
    "&emsp;&emsp;&emsp;&emsp;1) Create iceberg tables for input/output data in Hive catalog<br>\n",
    "&emsp;&emsp;&emsp;&emsp;2) Read input data from input iceberg table from S3 into a dataframe and cast to Dataset<br>\n",
    "&emsp;&emsp;&emsp;&emsp;3) Perform transformations on top of dataset <br>\n",
    "&emsp;&emsp;&emsp;&emsp;4) Store intermediate results in HDFS <br>\n",
    "&emsp;&emsp;&emsp;&emsp;5) Do 3 and 4 for all N steps<br>\n",
    "&emsp;&emsp;&emsp;&emsp;6) Combine all HDFS output and write to S3 location in Iceberg table format (final destination)<br>\n",
    "&emsp;&emsp;&emsp;&emsp;7) (Optional) Drop input and output Iceberg tables but keep the S3 data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f04d44",
   "metadata": {},
   "source": [
    "<a id=\"sparksqldftest\"></a>\n",
    "## Hybrid tests with Dataframes + SparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56979211",
   "metadata": {},
   "source": [
    "In our previous cases, the input was a Dataframe but the resultant table was created and inserted to via SparkSQL. Let us try an hybrid implementation where input is dataframe and the output table is created from SparkSQL. However, lets do the insert via DF operation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b573feb5",
   "metadata": {},
   "source": [
    "### Creating a new iceberg table \n",
    "\n",
    "There must be other ways to achieve this with non-SQL syntax (for type safety)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0e49d6fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b863530c82a4527a8c283b20b63b0b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res28: org.apache.spark.sql.DataFrame = []\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE iceberg_table_sparksqldf22 (id bigint,\n",
    "                                       month bigint,\n",
    "                                       sk bigint,\n",
    "                                       txt struct<key1:string>,\n",
    "                                       uuid string,\n",
    "                                       year string,\n",
    "                                       modified_timestamp timestamp,\n",
    "                                       z string,\n",
    "                                       `schema_v` string,   --had to rename these columns due to error\n",
    "                                       `data_v` string,\n",
    "                                       trade_dt string)\n",
    "USING iceberg\n",
    "OPTIONS ( 'write.object-storage.enabled'=true,\n",
    "          'write.object-storage.path'='s3://vasveena-test-hmswarehouse/')\n",
    "PARTITIONED BY (z,`schema_v`,`data_v`,trade_dt)\n",
    "LOCATION 's3://vasveena-test-demo/iceberg/tables/catalog/example_iceberg_perf_test_sparksqldf22'\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd4ded9",
   "metadata": {},
   "source": [
    "### Bulk inserting data into the iceberg table\n",
    "\n",
    "This time lets insert using insertInto on input DF. \n",
    "\n",
    "#### Gotcha \n",
    "\n",
    "Partition fields of the input dataframe should be at end to match with the output iceberg table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "695d6f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe2629448314b5eb7d2ea956829fe1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: Long = 1664066758504441\n",
      "duration: String = 160.410501777seconds\n"
     ]
    }
   ],
   "source": [
    "val t1 = System.nanoTime\n",
    "\n",
    "//order by clause is needed to avoid error \"Caused by: java.lang.IllegalStateException: Already closed files for partition: z=c/schema_v=v1/data_v=v2/trade_dt=2021-04-02\"\n",
    "input_df4.orderBy(\"z\",\"`schema-v`\",\"`data-v`\",\"trade_dt\").write.mode(\"overwrite\").insertInto(\"iceberg_table_sparksqldf22\")\n",
    "\n",
    "val duration = (System.nanoTime - t1) / 1e9d + \"seconds\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8bc14c",
   "metadata": {},
   "source": [
    "### Reading data from table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "dd98133d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51aa9c1e82c34c3bb0d28fd0dc3dea5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|      id|month|      sk|txt|                uuid|year| modified_timestamp|  z|schema_v|data_v|  trade_dt|\n",
      "+--------+-----+--------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|38000025|    3|38000025|[L]|60098a8c-0bf9-47f...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|38000031|    3|38000031|[R]|a4bf6610-36d0-43b...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|38000043|    6|38000043|[D]|495d0f6d-dc23-4e6...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|38000069|    6|38000069|[D]|9b85c17c-74bd-403...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|38000075|   12|38000075|[J]|64ab0d92-7084-4ca...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "+--------+-----+--------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(\"iceberg_table_sparksqldf22\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3ced4d",
   "metadata": {},
   "source": [
    "### Listing S3 locations\n",
    "\n",
    "Now lets recursively check one of the hashes under \"write.object-storage.enabled\" where the Datafrane + SparkSQL table data resides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d8896864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-18 00:27:11  163906720 2b857635/catalog/example_iceberg_perf_test_sparksqldf22/z=e/schema_v=v1/data_v=v2/trade_dt=2021-04-02/00014-293-0f52bdc9-82ad-480a-bd2a-bde2e25d22ed-00001.parquet\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "aws s3 ls \"s3://vasveena-test-hmswarehouse/2b857635/catalog/example_iceberg_perf_test_sparksqldf22/\" --recursive | head -n 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08f7468",
   "metadata": {},
   "source": [
    "Listing metadata on S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dca527c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-18 00:24:27       2919 00000-09eac564-e370-4c54-931b-0fd1579a943a.metadata.json\n",
      "2021-05-18 00:27:31       3948 00001-e3bae3ac-cd00-46a3-a097-9075c4a88992.metadata.json\n",
      "2021-05-18 00:27:31       9499 c88893f2-523c-4a14-879e-827f8b7a61ca-m0.avro\n",
      "2021-05-18 00:27:31       3571 snap-868207853291015711-1-c88893f2-523c-4a14-879e-827f8b7a61ca.avro\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "aws s3 ls \"s3://vasveena-test-demo/iceberg/tables/catalog/example_iceberg_perf_test_sparksqldf22/metadata/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d61e61",
   "metadata": {},
   "source": [
    "<a id=\"sparksqldstest\"></a>\n",
    "## Hybrid tests with Dataset + SparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5482d377",
   "metadata": {},
   "source": [
    "Now lets try something similar with dataset. We will create a dataset object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba00ac8e",
   "metadata": {},
   "source": [
    "### Creating Dataset from Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5a376037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64faa64823fb47c09b28931cf790fcad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.types._\n",
      "import org.apache.spark.sql.Dataset\n",
      "import org.apache.spark.sql.Row\n",
      "defined class TxtStruct\n",
      "defined class Random\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.Dataset\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "/* val schema = (new StructType()\n",
    "               .add(\"id\", IntegerType)\n",
    "               .add(\"month\",IntegerType)\n",
    "               .add(\"sk\",IntegerType)\n",
    "               .add(\"txt\",new StructType()\n",
    "                    .add(\"key1\",StringType))\n",
    "               .add(\"uuid\",StringType)\n",
    "               .add(\"year\",StringType)\n",
    "               .add(\"modified_timestamp\",TimestampType)\n",
    "               .add(\"z\",StringType)\n",
    "               .add(\"schema_v\", StringType)\n",
    "               .add(\"data_v\", StringType)\n",
    "               .add(\"trade_dt\", StringType)) */\n",
    "\n",
    "case class TxtStruct(key1: String)\n",
    "\n",
    "case class Random(id: BigInt, \n",
    "                  month: BigInt, \n",
    "                  sk: BigInt, \n",
    "                  txt: Seq[TxtStruct], \n",
    "                  uuid: String, \n",
    "                  year: String,\n",
    "                  modified_timestamp: java.sql.Timestamp, \n",
    "                  z: String, \n",
    "                  schema_v: String, \n",
    "                  data_v: String, \n",
    "                  trade_dt: String)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c578718a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e02b6c8a538445c9b56a6010d7640f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds_input: org.apache.spark.sql.Dataset[Random] = [id: bigint, month: bigint ... 9 more fields]\n"
     ]
    }
   ],
   "source": [
    "val ds_input = input_df4.select($\"id\",$\"month\",$\"sk\"\n",
    "                                //,$\"txt\".cast(\"array<struct<key1:string>>\")\n",
    "                                ,(array($\"txt\")) as \"txt\"\n",
    "                                ,$\"uuid\",$\"year\",$\"modified_timestamp\",$\"z\"\n",
    "                                ,$\"schema_v\",$\"data_v\",$\"trade_dt\").as[Random]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4a9560a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a2d01d1414546ef9731d55b9df3afdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- month: long (nullable = true)\n",
      " |-- sk: long (nullable = true)\n",
      " |-- txt: array (nullable = false)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- key1: string (nullable = true)\n",
      " |-- uuid: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- modified_timestamp: timestamp (nullable = true)\n",
      " |-- z: string (nullable = true)\n",
      " |-- schema_v: string (nullable = false)\n",
      " |-- data_v: string (nullable = false)\n",
      " |-- trade_dt: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds_input.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1ce2c98a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d9fc8aae804da593dda3c4668b0545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-------+-----+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|     id|month|     sk|  txt|                uuid|year| modified_timestamp|  z|schema_v|data_v|  trade_dt|\n",
      "+-------+-----+-------+-----+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|4000000|    3|4000000|[[E]]|6e505939-f5fd-4ab...|2019|2021-04-02 00:05:02|  9|      v1|    v2|2021-04-02|\n",
      "|4000001|    9|4000001|[[F]]|20486aca-2759-43f...|2019|2021-04-02 00:05:02|  d|      v1|    v2|2021-04-02|\n",
      "|4000002|   11|4000002|[[G]]|42962a21-a2dc-40d...|2019|2021-04-02 00:05:02|  d|      v1|    v2|2021-04-02|\n",
      "|4000003|    9|4000003|[[H]]|9841ad6d-1532-496...|2019|2021-04-02 00:05:02|  c|      v1|    v2|2021-04-02|\n",
      "|4000004|    4|4000004|[[I]]|ff1a855a-cced-495...|2019|2021-04-02 00:05:02|  4|      v1|    v2|2021-04-02|\n",
      "+-------+-----+-------+-----+--------------------+----+-------------------+---+--------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds_input.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bd15d8",
   "metadata": {},
   "source": [
    "### Creating a new iceberg table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1c3ad32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c295ac9e35bd4c23acde6929fe70965f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res61: org.apache.spark.sql.DataFrame = []\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE iceberg_table_sparksqlds66(id bigint,\n",
    "                                       month bigint,\n",
    "                                       sk bigint,\n",
    "                                       -- txt struct<key1:string>,\n",
    "                                       txt array<struct<key1:string>>,\n",
    "                                       uuid string,\n",
    "                                       year string,\n",
    "                                       modified_timestamp timestamp,\n",
    "                                       z string,\n",
    "                                       `schema_v` string,   --had to rename these columns due to error\n",
    "                                       `data_v` string,\n",
    "                                       trade_dt string)\n",
    "USING iceberg\n",
    "OPTIONS ( 'write.object-storage.enabled'=true,\n",
    "          'write.object-storage.path'='s3://vasveena-test-hmswarehouse/')\n",
    "PARTITIONED BY (z,`schema_v`,`data_v`,trade_dt)\n",
    "LOCATION 's3://vasveena-test-demo/iceberg/tables/catalog/example_iceberg_perf_test_sparksqlds66'\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876ac74e",
   "metadata": {},
   "source": [
    "### Bulk insert dataset to iceberg table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "885c8d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d8514a4af1456e996d2cb0982cf532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: Long = 1666192491945114\n",
      "duration: String = 175.917568684seconds\n"
     ]
    }
   ],
   "source": [
    "val t1 = System.nanoTime\n",
    "\n",
    "//order by clause is needed to avoid error \"Caused by: java.lang.IllegalStateException: Already closed files for partition: z=c/schema_v=v1/data_v=v2/trade_dt=2021-04-02\"\n",
    "ds_input.orderBy(\"z\",\"`schema-v`\",\"`data-v`\",\"trade_dt\").write.mode(\"overwrite\").insertInto(\"iceberg_table_sparksqlds66\")\n",
    "\n",
    "val duration = (System.nanoTime - t1) / 1e9d + \"seconds\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66173b4a",
   "metadata": {},
   "source": [
    "### Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bd08c072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cef74930b744cf183ce322274804867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-------+-----+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|     id|month|     sk|  txt|                uuid|year| modified_timestamp|  z|schema_v|data_v|  trade_dt|\n",
      "+-------+-----+-------+-----+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|6000003|    6|6000003|[[J]]|6ed8ea5f-1693-441...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|6000006|    7|6000006|[[M]]|d8c42b7a-168f-477...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|6000008|    2|6000008|[[O]]|dad5d203-faf0-413...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|6000046|    3|6000046|[[A]]|b8ec2d30-2ea8-414...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|6000052|   11|6000052|[[G]]|61cb5988-97c0-49e...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "+-------+-----+-------+-----+--------------------+----+-------------------+---+--------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(\"iceberg_table_sparksqlds66\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8166d2",
   "metadata": {},
   "source": [
    "### Listing S3 locations\n",
    "\n",
    "Now lets recursively check one of the hashes under \"write.object-storage.enabled\" where the Dataset + SparkSQL table data resides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b4f42a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-18 00:58:59  163824894 2d875426/catalog/example_iceberg_perf_test_sparksqlds66/z=d/schema_v=v1/data_v=v2/trade_dt=2021-04-02/00013-497-e87978c0-22cb-4940-bece-31fc29a6e684-00001.parquet\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "aws s3 ls \"s3://vasveena-test-hmswarehouse/2d875426/catalog/example_iceberg_perf_test_sparksqlds66/\" --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b80290",
   "metadata": {},
   "source": [
    "Listing metadata on S3 location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "83651fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-18 00:56:20       3053 00000-0fdb41e8-e76e-4a2f-b685-c08c6e69886d.metadata.json\n",
      "2021-05-18 00:59:15       4086 00001-ef435d4d-0c42-46aa-b211-f1ac6c5ef494.metadata.json\n",
      "2021-05-18 00:59:15       9547 8826bf95-fa26-4470-8e5c-d5c1ee46df89-m0.avro\n",
      "2021-05-18 00:59:15       3571 snap-2984059586568082337-1-8826bf95-fa26-4470-8e5c-d5c1ee46df89.avro\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "aws s3 ls \"s3://vasveena-test-demo/iceberg/tables/catalog/example_iceberg_perf_test_sparksqlds66/metadata/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3305dea",
   "metadata": {},
   "source": [
    "<a id=\"udf\"></a>\n",
    "## Calling UDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14414d9e",
   "metadata": {},
   "source": [
    "Since we are able to read data from Iceberg table into Dataframe/Dataset, we can also manipulate data using UDF / Dataset APIs such as copy or flatMapGroups. These APIs are heavily leveraged by FINRA Catlinker. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af74bf60",
   "metadata": {},
   "source": [
    "### Dataset APIs\n",
    "\n",
    "Let us first try Dataset APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "83d0a09f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "578ed6cc8b2f4418a6697a6cada0de0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds: org.apache.spark.sql.Dataset[Random] = [id: bigint, month: bigint ... 9 more fields]\n"
     ]
    }
   ],
   "source": [
    "val ds = spark.table(\"iceberg_table_sparksqlds66\").as[Random]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c0f149ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87580697060a4690aa986f5dbb31f250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- month: long (nullable = true)\n",
      " |-- sk: long (nullable = true)\n",
      " |-- txt: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- key1: string (nullable = true)\n",
      " |-- uuid: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- modified_timestamp: timestamp (nullable = true)\n",
      " |-- z: string (nullable = true)\n",
      " |-- schema_v: string (nullable = true)\n",
      " |-- data_v: string (nullable = true)\n",
      " |-- trade_dt: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73ccc1e",
   "metadata": {},
   "source": [
    "We can also create a dataset by creating a new Iceberg table in case the table is deleted from metastore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db25f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val ds = spark.read.format(\"iceberg\").option(\"path\", \"s3://vasveena-test-demo/iceberg/tables/catalog/example_iceberg_perf_test_sparksqlds66\").as[Random]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb89954",
   "metadata": {},
   "source": [
    "#### Creating a random function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fffcf64a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ab42c8cf97478a9900b386f193ade2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "somefunc: (trade_dt: String, values: Iterator[Random])TraversableOnce[(String, Random)]\n"
     ]
    }
   ],
   "source": [
    "def somefunc(trade_dt:String, values:Iterator[Random]): TraversableOnce[(String,Random)] = {\n",
    "  println(\"Print from somefunc\")\n",
    "  return values.map(x => (trade_dt, x))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e596a3",
   "metadata": {},
   "source": [
    "Use flatMapGroups to call somefunc over trade_dt groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "153dcea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a2b4bd0c16e4bc49c3fff43769f1ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "udf_ds: org.apache.spark.sql.DataFrame = [trade_dt: string, vector: struct<id: decimal(38,0), month: decimal(38,0) ... 9 more fields>]\n",
      "+----------+--------------------+\n",
      "|  trade_dt|              vector|\n",
      "+----------+--------------------+\n",
      "|2021-04-02|[44000000, 3, 440...|\n",
      "|2021-04-02|[36291570, 6, 362...|\n",
      "|2021-04-02|[44000028, 7, 440...|\n",
      "|2021-04-02|[36291582, 11, 36...|\n",
      "|2021-04-02|[44000032, 6, 440...|\n",
      "+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val udf_ds = (ds.groupByKey(t => t.trade_dt)\n",
    "              .flatMapGroups(somefunc).withColumnRenamed(\"_1\",\"trade_dt\").withColumnRenamed(\"_2\",\"vector\"))\n",
    "\n",
    "udf_ds.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cc7cbc",
   "metadata": {},
   "source": [
    "#### We can write this result back into a new Iceberg table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "381813d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ccd637126149609505b655b4bf514f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- trade_dt: string (nullable = true)\n",
      " |-- vector: struct (nullable = true)\n",
      " |    |-- id: decimal(38,0) (nullable = true)\n",
      " |    |-- month: decimal(38,0) (nullable = true)\n",
      " |    |-- sk: decimal(38,0) (nullable = true)\n",
      " |    |-- txt: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- key1: string (nullable = true)\n",
      " |    |-- uuid: string (nullable = true)\n",
      " |    |-- year: string (nullable = true)\n",
      " |    |-- modified_timestamp: timestamp (nullable = true)\n",
      " |    |-- z: string (nullable = true)\n",
      " |    |-- schema_v: string (nullable = true)\n",
      " |    |-- data_v: string (nullable = true)\n",
      " |    |-- trade_dt: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udf_ds.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "32bf68ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8bea9dc96b5497789b4ffcbc8465f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res84: org.apache.spark.sql.DataFrame = []\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE iceberg_table_sparksqldsudf22 (\n",
    "                                    trade_dt string,\n",
    "                                    vector struct<id: bigint,\n",
    "                                       month:bigint,\n",
    "                                       sk:bigint,\n",
    "                                       txt:array<struct<key1:string>>,\n",
    "                                       uuid:string,\n",
    "                                       year:string,\n",
    "                                       modified_timestamp:timestamp,\n",
    "                                       z:string,\n",
    "                                       schema_v:string,   --had to rename these columns due to error\n",
    "                                       data_v:string,\n",
    "                                       trade_dt:string>)\n",
    "USING iceberg\n",
    "OPTIONS ( 'write.object-storage.enabled'=true,\n",
    "          'write.object-storage.path'='s3://vasveena-test-hmswarehouse/')\n",
    "LOCATION 's3://vasveena-test-demo/iceberg/tables/catalog/example_iceberg_perf_test_sparksqldsudf22'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "459ab3f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43111aa869254e3fa5ef0cf68537d3ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: Long = 1667999152218613\n",
      "duration: String = 1172.648418599seconds\n"
     ]
    }
   ],
   "source": [
    "val t1 = System.nanoTime\n",
    "\n",
    "//no order by clause needed here since the dataset does not have a partition\n",
    "udf_ds.write.mode(\"overwrite\").insertInto(\"iceberg_table_sparksqldsudf22\")\n",
    "\n",
    "val duration = (System.nanoTime - t1) / 1e9d + \"seconds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fd89e3ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d6aea13999346c1897e9748e2f359a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|  trade_dt|              vector|\n",
      "+----------+--------------------+\n",
      "|2021-04-02|[18271721, 2, 182...|\n",
      "|2021-04-02|[24120637, 3, 241...|\n",
      "|2021-04-02|[18271722, 12, 18...|\n",
      "|2021-04-02|[24120638, 4, 241...|\n",
      "|2021-04-02|[18271728, 2, 182...|\n",
      "+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(\"iceberg_table_sparksqldsudf22\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c057b396",
   "metadata": {},
   "source": [
    "#### Listing warehouse for the above write under one of the hashes\n",
    "\n",
    "```\n",
    "aws s3 ls s3://vasveena-test-hmswarehouse/53e6b258/catalog/example_iceberg_perf_test_sparksqldsudf22/\n",
    "\n",
    "2021-05-17 21:34:34 2640936849 00001-637-c6e4e0fe-a115-4b3d-b083-0899c7d490c1-00001.parquet\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e35408d",
   "metadata": {},
   "source": [
    "#### We can also write into an HDFS location leveraged by FINRA for storing checkpoint data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c5ad2b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7131a545cdb4b62b07c4d1f84ce63d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: Long = 1672027455932338\n",
      "duration: String = 840.13282666seconds\n"
     ]
    }
   ],
   "source": [
    "val t1 = System.nanoTime\n",
    "\n",
    "//write to HDFS\n",
    "udf_ds.write.mode(\"overwrite\").parquet(\"/user/hadoop/checkpoint/example_iceberg_perf_test_sparksqldsudf22/\")\n",
    "\n",
    "val duration = (System.nanoTime - t1) / 1e9d + \"seconds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7c6b3625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c291422c95477eb011975688ef784d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfsdf: org.apache.spark.sql.DataFrame = [trade_dt: string, vector: struct<id: decimal(38,0), month: decimal(38,0) ... 9 more fields>]\n",
      "+----------+--------------------+\n",
      "|  trade_dt|              vector|\n",
      "+----------+--------------------+\n",
      "|2021-04-02|[6000003, 6, 6000...|\n",
      "|2021-04-02|[28719690, 1, 287...|\n",
      "|2021-04-02|[6000006, 7, 6000...|\n",
      "|2021-04-02|[28719705, 6, 287...|\n",
      "|2021-04-02|[6000008, 2, 6000...|\n",
      "+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val hdfsdf = spark.read.parquet(\"/user/hadoop/checkpoint/example_iceberg_perf_test_sparksqldsudf22/\")\n",
    "hdfsdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d774d840",
   "metadata": {},
   "source": [
    "#### Listing HDFS location within EMR \n",
    "\n",
    "```\n",
    "\n",
    "[hadoop@ip-172-31-41-141 ~]$ hdfs dfs -ls /user/hadoop/checkpoint/example_iceberg_perf_test_sparksqldsudf22/\n",
    "Found 3 items\n",
    "-rw-r--r--   1 livy hadoop          0 2021-05-18 02:51 /user/hadoop/checkpoint/example_iceberg_perf_test_sparksqldsudf22/_SUCCESS\n",
    "-rw-r--r--   1 livy hadoop       1514 2021-05-18 02:38 /user/hadoop/checkpoint/example_iceberg_perf_test_sparksqldsudf22/part-00000-d72eb454-b9fd-4530-8b41-6d291cd0a20e-c000.snappy.parquet\n",
    "-rw-r--r--   1 livy hadoop 5419772564 2021-05-18 02:51 /user/hadoop/checkpoint/example_iceberg_perf_test_sparksqldsudf22/part-00001-d72eb454-b9fd-4530-8b41-6d291cd0a20e-c000.snappy.parquet\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab009ccf",
   "metadata": {},
   "source": [
    "### Dataframe UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ae509929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8010e46bf5d4090a849cf87b741b135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df: org.apache.spark.sql.DataFrame = [id: bigint, month: bigint ... 9 more fields]\n"
     ]
    }
   ],
   "source": [
    "val df = spark.table(\"iceberg_table_sparksqlds66\") //Reading above table into a Dataframe. Can use any method to load dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0886496",
   "metadata": {},
   "source": [
    "Creating a test UDF to call on this dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "30f14461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30adadb0b70b4ff598a8eabdbe2c2592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "someudf: java.math.BigDecimal => String\n"
     ]
    }
   ],
   "source": [
    "def someudf = (sk:java.math.BigDecimal) => {\n",
    "  println(\"Print from UDF\")\n",
    "  ((sk+\"1\")).toString\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ec3ced15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "745b2d5a717b44f48506168251ceb8e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.functions.udf\n",
      "someUDF: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$5520/989180592@3406cac9,StringType,List(Some(class[value[0]: decimal(38,18)])),None,true,true)\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.udf\n",
    "val someUDF = udf(someudf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0c52e5be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e108388e6b8496aa9582520fa4a5991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "udf_df: org.apache.spark.sql.DataFrame = [id: bigint, month: bigint ... 10 more fields]\n",
      "+-------+-----+-------+-----+--------------------+----+-------------------+---+--------+------+----------+--------------------+\n",
      "|     id|month|     sk|  txt|                uuid|year| modified_timestamp|  z|schema_v|data_v|  trade_dt|              udfcol|\n",
      "+-------+-----+-------+-----+--------------------+----+-------------------+---+--------+------+----------+--------------------+\n",
      "|6000003|    6|6000003|[[J]]|6ed8ea5f-1693-441...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|6000003.000000000...|\n",
      "|6000006|    7|6000006|[[M]]|d8c42b7a-168f-477...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|6000006.000000000...|\n",
      "|6000008|    2|6000008|[[O]]|dad5d203-faf0-413...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|6000008.000000000...|\n",
      "|6000046|    3|6000046|[[A]]|b8ec2d30-2ea8-414...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|6000046.000000000...|\n",
      "|6000052|   11|6000052|[[G]]|61cb5988-97c0-49e...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|6000052.000000000...|\n",
      "+-------+-----+-------+-----+--------------------+----+-------------------+---+--------+------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val udf_df = df.withColumn(\"udfcol\",someUDF($\"id\"))\n",
    "udf_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c614922c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be87ea1ba8c94d24ab515cf0af43d614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res107: org.apache.spark.sql.DataFrame = []\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE iceberg_table_sparksqldfudf24(id bigint,\n",
    "                                       month bigint,\n",
    "                                       sk bigint,\n",
    "                                       -- txt struct<key1:string>,\n",
    "                                       txt array<struct<key1:string>>,\n",
    "                                       uuid string,\n",
    "                                       year string,\n",
    "                                       modified_timestamp timestamp,\n",
    "                                       udfcol string,\n",
    "                                       z string,\n",
    "                                       `schema_v` string,   --had to rename these columns due to error\n",
    "                                       `data_v` string,\n",
    "                                       trade_dt string)\n",
    "USING iceberg\n",
    "OPTIONS ( 'write.object-storage.enabled'=true,\n",
    "          'write.object-storage.path'='s3://vasveena-test-hmswarehouse/')\n",
    "PARTITIONED BY (z,`schema_v`,`data_v`,trade_dt)\n",
    "LOCATION 's3://vasveena-test-demo/iceberg/tables/catalog/example_iceberg_perf_test_sparksqldfudf24'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5cae5b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf543d84569429c930458a27a060ef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- month: long (nullable = true)\n",
      " |-- sk: long (nullable = true)\n",
      " |-- txt: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- key1: string (nullable = true)\n",
      " |-- uuid: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- modified_timestamp: timestamp (nullable = true)\n",
      " |-- z: string (nullable = true)\n",
      " |-- schema_v: string (nullable = true)\n",
      " |-- data_v: string (nullable = true)\n",
      " |-- trade_dt: string (nullable = true)\n",
      " |-- udfcol: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udf_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9529ce33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02fb9fbbc43644349d01db0675a9ad40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "udf_df.select($\"id\",$\"month\",$\"sk\",$\"txt\",$\"uuid\",$\"year\",$\"modified_timestamp\",$\"udfcol\",$\"z\",$\"schema_v\",$\"data_v\",$\"trade_dt\").orderBy(\"z\",\"`schema_v`\",\"`data_v`\",\"trade_dt\").write.mode(\"overwrite\").insertInto(\"iceberg_table_sparksqldfudf24\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335d92b4",
   "metadata": {},
   "source": [
    "#### Listing warehouse location for above table on one of the hashes\n",
    "\n",
    "```\n",
    "\n",
    "aws s3 ls s3://vasveena-test-hmswarehouse/18742155/catalog/example_iceberg_perf_test_sparksqldfudf24/ --recursive\n",
    "2021-05-17 23:09:15  183401535 18742155/catalog/example_iceberg_perf_test_sparksqldfudf24/z=c/schema_v=v1/data_v=v2/trade_dt=2021-04-02/00012-799-c1b939a2-e9c1-4aab-92fc-5d6d12f1718d-00001.parquet\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279f7ffe",
   "metadata": {},
   "source": [
    "### Listing table S3 location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8ab4e29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-18 02:52:21       3159 iceberg/tables/catalog/example_iceberg_perf_test_sparksqldfudf24/metadata/00000-68ad005f-e320-471c-a8fc-6bd959ad9018.metadata.json\n",
      "2021-05-18 03:09:21       4198 iceberg/tables/catalog/example_iceberg_perf_test_sparksqldfudf24/metadata/00001-78697895-d347-4c96-863a-4c8e06a43652.metadata.json\n",
      "2021-05-18 03:09:21       9880 iceberg/tables/catalog/example_iceberg_perf_test_sparksqldfudf24/metadata/87e22fa9-87da-4dc5-8c34-b06e8eb25db3-m0.avro\n",
      "2021-05-18 03:09:21       3575 iceberg/tables/catalog/example_iceberg_perf_test_sparksqldfudf24/metadata/snap-4899277091392625360-1-87e22fa9-87da-4dc5-8c34-b06e8eb25db3.avro\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "aws s3 ls \"s3://vasveena-test-demo/iceberg/tables/catalog/example_iceberg_perf_test_sparksqldfudf24/\" --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69d4c34",
   "metadata": {},
   "source": [
    "Result is as expected. So we should be able to transform intermediate data and write the output to an existing Iceberg table or onto HDFS staging location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f33826c",
   "metadata": {},
   "source": [
    "<a id=\"knownissues\"></a>\n",
    "## Known Issues\n",
    "\n",
    "### 1) Already closed files for partition\n",
    "\n",
    "While bulk inserting data to Iceberg tables, if the table size is big, then more than 50% of the time, we may run into this failure. Smaller tables would work fine. \n",
    "\n",
    "```\n",
    "Caused by: java.lang.IllegalStateException: Already closed files for partition: z=c/schema-v=v1/data-v=v2/trade_dt=2021-04-02\n",
    "  at org.apache.iceberg.io.PartitionedWriter.write(PartitionedWriter.java:69)\n",
    "  at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$7(WriteToDataSourceV2Exec.scala:441)\n",
    "  at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\n",
    "  at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:477)\n",
    "  at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:385)\n",
    "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
    "  at org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
    "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
    "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
    "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
    "  ... 3 more\n",
    "\n",
    "```\n",
    "\n",
    "This is due to the following issue:\n",
    "\n",
    "https://github.com/apache/iceberg/issues/508 <br>\n",
    "https://lists.apache.org/thread.html/e54d90b11d94e1bf93f1c750939271f79a96dfa94ef9c2c10f586546@%3Cdev.iceberg.apache.org%3E\n",
    "\n",
    "To workaround, all partition fields should be sorted prior to write to avoid this issue for large datasets (especially during bulk insert). \n",
    "\n",
    "```\n",
    "ds_input.orderBy(\"z\",\"schema-v\",\"data-v\",\"trade_dt\").write.mode(\"overwrite\").insertInto(\"iceberg_table_sparksqlds6\") → like this\n",
    "```\n",
    "\n",
    "### 2) Unquoted Identifier\n",
    "\n",
    "While writing data from Dataframe/Dataset to Iceberg table, ran into the below issue since field names had “-” in them. \n",
    "\n",
    "An error was encountered:\n",
    "\n",
    "```\n",
    "org.apache.spark.sql.catalyst.parser.ParseException:\n",
    "Possibly unquoted identifier schema-v detected. Please consider quoting it with back-quotes as `schema-v`(line 1, pos 6)\n",
    "\n",
    "== SQL ==\n",
    "schema-v\n",
    "------^^^\n",
    "\n",
    "```\n",
    "\n",
    "It kept complaining of this error even in the presence of quoting (`) on source and destination. So, had to change \"-\" to \"_\". \n",
    "\n",
    "### 3) Drop table does not work \n",
    "\n",
    "Unable to drop tables from Iceberg HMS catalog. Drop will happen successfully but the table will still be present in the metastore. Spark by default does a table exist check. \n",
    "\n",
    "Issue:\n",
    "https://github.com/apache/iceberg/issues/2374\n",
    "\n",
    "Workaround attempt.\n",
    "\n",
    "Start spark-shell in EMR master node using following command (usually JARs are loaded when they are on classpath but I had to explicitly specify iceberg JAR for some reason)\n",
    "\n",
    "spark-shell --conf spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog --conf spark.sql.catalog.spark_catalog.type=hive --conf spark.sql.catalog.spark_catalog.uri=thrift://localhost:9083 —conf spark.hadoop.hive.metastore.uris=thrift://localhost:9083 —jars /usr/lib/spark/jars/iceberg-spark3-0.11.0.jar\n",
    "\n",
    "import org.apache.iceberg.spark.SparkSessionCatalog\n",
    "import org.apache.spark.sql.connector.catalog.Identifier\n",
    "val db = spark.catalog.currentDatabase\n",
    "spark.sessionState.catalogManager.catalog(\"hive\").asInstanceOf[SparkSessionCatalog[_]].dropTable(db, table = \"iceberg_table_sparksql\", ignoreIfNotExists = false, purge = true)\n",
    "\n",
    "The above does not work. It expects an Identifier (https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/connector/catalog/Identifier.html) in dropTable API. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2ac64f",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "## Key takeaways\n",
    "\n",
    "1) Destination Iceberg table needs to be created prior to write for ObjectStoreLocationProvider to work as expected. It does not matter if the write is performed from SparkSQL/DataframeWriter. \n",
    "\n",
    "2) Input S3/HDFS data can be read into a dataset (cast) in Iceberg format. Transformations can be performed on top of this dataset (tested UDF and a dataset API) and the transformed dataset can be written to the output Iceberg table or temporary HDFS location.\n",
    "\n",
    "3) Direct Parquet write (without Iceberg) took ~ 95 seconds for 100M records. Bulk insert in iceberg for same amount of data took anywhere from ~140-160 seconds (with sort) using SparkSQL. It took ~160 seconds with insertInto API using Dataframe/Datasets (with sort). FINRA’s workloads always does bulk inserts i.e., no incremental processing so this increase in write cost (sort with TBs of data) should be taken into consideration until we workaround the issue \"Already closed files for partition\".\n",
    "\n",
    "4) There are some corner case errors we are running into which may impact customer experience (some of the gotchas stated below). For example, all partition columns must be sorted prior to write to avoid a random error. This could become costly for TBs of data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f61362a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

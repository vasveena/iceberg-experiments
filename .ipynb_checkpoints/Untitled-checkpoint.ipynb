{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30470b8e",
   "metadata": {},
   "source": [
    "# Spark-Iceberg Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac6dba5",
   "metadata": {},
   "source": [
    "# Index\n",
    "\n",
    "1) [Configuring Iceberg](#configure_iceberg) <br>\n",
    "2) [Evaluating Iceberg for Spark](#test) <br>\n",
    "&emsp;&emsp;&emsp;&emsp;a) [Regular Dataframe behavior](#normaldftest) <br>\n",
    "&emsp;&emsp;&emsp;&emsp;b) [Pure Dataframe + Iceberg](#dftest) <br>\n",
    "&emsp;&emsp;&emsp;&emsp;c) [SparkSQL + Iceberg](#sparksqltest) <br>\n",
    "3) [Hybrid tests with Dataframe and Datasets](#hybrid) <br>\n",
    "&emsp;&emsp;&emsp;&emsp;a) [SparkSQL + Iceberg + Dataframe](#sparksqldftest) <br>\n",
    "&emsp;&emsp;&emsp;&emsp;b) [SparkSQL + Iceberg + Dataset](#sparksqldstest) <br>\n",
    "4) [Perform transformations with UDFs and Dataset APIs](#udf) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b62ad1",
   "metadata": {},
   "source": [
    "<a id=\"configure_iceberg\"></a>\n",
    "# Configuring Iceberg on Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "435ac1d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars.packages': 'org.apache.iceberg:iceberg-spark3-runtime:0.11.0', 'spark.sql.extensions': 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions', 'spark.sql.catalog.spark_catalog': 'org.apache.iceberg.spark.SparkSessionCatalog', 'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.sql.catalog.local': 'org.apache.iceberg.spark.SparkCatalog', 'spark.sql.catalog.local.type': 'hadoop', 'spark.sql.catalog.local.warehouse': 's3://vasveena-test-demo/iceberg/catalog/tables/'}, 'proxyUser': 'user_vasveena', 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":  { \n",
    "             \"spark.jars.packages\":\"org.apache.iceberg:iceberg-spark3-runtime:0.11.0\",\n",
    "             \"spark.sql.extensions\":\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "             \"spark.sql.catalog.spark_catalog\":\"org.apache.iceberg.spark.SparkSessionCatalog\",\n",
    "             \"spark.sql.catalog.spark_catalog.type\":\"hive\",\n",
    "             \"spark.sql.catalog.local\":\"org.apache.iceberg.spark.SparkCatalog\",\n",
    "             \"spark.sql.catalog.local.type\":\"hadoop\",\n",
    "             \"spark.sql.catalog.local.warehouse\":\"s3://vasveena-test-demo/iceberg/catalog/tables/\"\n",
    "           } \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfb0fe8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f99cbabe3184a3288f154fe0fc17da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>8</td><td>application_1619633879001_0010</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-41-141.ec2.internal:20888/proxy/application_1619633879001_0010/\" >Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-44-57.ec2.internal:8042/node/containerlogs/container_1619633879001_0010_01_000001/livy\" >Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res1: String = 3.0.1-amzn-0\n"
     ]
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb6af0e",
   "metadata": {},
   "source": [
    "<a id=\"test\"></a>\n",
    "# Evaluating Iceberg for Spark\n",
    "<a id=\"normaldftest\"></a>\n",
    "## Regular Dataframe Read/Write test with Parquet Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b309df4",
   "metadata": {},
   "source": [
    "### Read Input data from S3\n",
    "\n",
    "Let's read our 100M record dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe1ebdfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d774ce900064ebeaf4523624389128a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_df: org.apache.spark.sql.DataFrame = [id: bigint, month: bigint ... 5 more fields]\n",
      "res2: Long = 100000000\n"
     ]
    }
   ],
   "source": [
    "val input_df = spark.read.parquet(\"s3://neilawstmp2/tmp/hudi-perf/input/\")\n",
    "input_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f476f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6297e3efee0549e9b6c274055aa27c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_df.createOrReplaceTempView(\"inputdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0e0dc9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30442c4538ac46d698aa1a890f6221f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- month: long (nullable = true)\n",
      " |-- sk: long (nullable = true)\n",
      " |-- txt: struct (nullable = true)\n",
      " |    |-- key1: string (nullable = true)\n",
      " |-- uuid: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- modified_timestamp: timestamp (nullable = true)\n",
      "\n",
      "res7: Int = 45\n"
     ]
    }
   ],
   "source": [
    "input_df.printSchema()\n",
    "input_df.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7228592b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "aws s3 ls s3://neilawstmp2/tmp/hudi-perf/input/ | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c01767",
   "metadata": {},
   "source": [
    "### Transform input data\n",
    "Performing some transformations on the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc55a678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "786f211b2a3a4923b4717e0373dd7a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.functions._\n",
      "input_df2: org.apache.spark.sql.DataFrame = [id: bigint, month: bigint ... 9 more fields]\n",
      "+-------+-----+-------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|     id|month|     sk|txt|                uuid|year| modified_timestamp|  z|schema-v|data-v|  trade_dt|\n",
      "+-------+-----+-------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|4000000|    3|4000000|[E]|6e505939-f5fd-4ab...|2019|2021-04-02 00:05:02|  9|      v1|    v2|2021-04-02|\n",
      "|4000001|    9|4000001|[F]|20486aca-2759-43f...|2019|2021-04-02 00:05:02|  d|      v1|    v2|2021-04-02|\n",
      "|4000002|   11|4000002|[G]|42962a21-a2dc-40d...|2019|2021-04-02 00:05:02|  d|      v1|    v2|2021-04-02|\n",
      "|4000003|    9|4000003|[H]|9841ad6d-1532-496...|2019|2021-04-02 00:05:02|  c|      v1|    v2|2021-04-02|\n",
      "|4000004|    4|4000004|[I]|ff1a855a-cced-495...|2019|2021-04-02 00:05:02|  4|      v1|    v2|2021-04-02|\n",
      "+-------+-----+-------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val input_df2=(input_df.withColumn(\"z\", substring(md5(concat($\"id\")),1,1))\n",
    "                       .withColumn(\"schema-v\", lit(\"v1\")).withColumn(\"data-v\", lit(\"v2\"))\n",
    "                       .withColumn(\"trade_dt\", substring($\"modified_timestamp\",1,10)))\n",
    "input_df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a2901f",
   "metadata": {},
   "source": [
    "### Write data back to S3 in normal Parquet format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c4324b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71ae3623a94b498da316069f55f675f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: Long = 639228646763777\n",
      "s3_location: String = s3://vasveena-test-demo/temp6/parquet-perf/catalog/example_iceberg_perf_test_6\n",
      "duration: String = 74.901509932 seconds\n"
     ]
    }
   ],
   "source": [
    "val t1 = System.nanoTime\n",
    "\n",
    "val s3_location=s\"s3://vasveena-test-demo/temp6/parquet-perf/catalog/example_iceberg_perf_test_6\"\n",
    "\n",
    "input_df2.write.mode(\"OVERWRITE\").partitionBy(\"z\",\"schema-v\",\"data-v\",\"trade_dt\").parquet(s3_location)\n",
    "\n",
    "val duration = (System.nanoTime - t1) / 1e9d + \" seconds\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737e35f4",
   "metadata": {},
   "source": [
    "### Write data back to S3 in Spark table format (Parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "80442d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0afcb5932fb34ba4a81d709ea8349451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: Long = 639530210694707\n",
      "s3_location_ndf: String = s3://vasveena-test-demo/iceberg/tables/catalog/example_iceberg_perf_test_normaldf\n",
      "duration: String = 97.160976689seconds\n"
     ]
    }
   ],
   "source": [
    "val t1 = System.nanoTime\n",
    "\n",
    "val s3_location_ndf=s\"s3://vasveena-test-demo/iceberg/tables/catalog/example_iceberg_perf_test_normaldf\"\n",
    "\n",
    "(input_df2.write.mode(\"OVERWRITE\")\n",
    "    .option(\"path\", s3_location_ndf)\n",
    "    .partitionBy(\"z\",\"schema-v\",\"data-v\",\"trade_dt\").format(\"parquet\")\n",
    "    .saveAsTable(\"iceberg_table_normaldf\"))\n",
    "\n",
    "val duration = (System.nanoTime - t1) / 1e9d + \"seconds\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34c24fe",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Reading output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "805348b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "150e599c8bbc4b21bc976e0885b66c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading direct Parquet output\n",
      "+-------+-----+-------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|     id|month|     sk|txt|                uuid|year| modified_timestamp|  z|schema-v|data-v|  trade_dt|\n",
      "+-------+-----+-------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|4000011|    2|4000011|[P]|dbf28c6d-6f94-449...|2019|2021-04-02 00:05:02|  3|      v1|    v2|2021-04-02|\n",
      "|4000017|    1|4000017|[V]|96fa033f-0fb0-4d5...|2019|2021-04-02 00:05:02|  3|      v1|    v2|2021-04-02|\n",
      "|4000047|   10|4000047|[Z]|93c488d8-b882-442...|2019|2021-04-02 00:05:02|  3|      v1|    v2|2021-04-02|\n",
      "|4000051|    2|4000051|[D]|4464b11b-14aa-4ca...|2019|2021-04-02 00:05:02|  3|      v1|    v2|2021-04-02|\n",
      "|4000071|    4|4000071|[X]|bdb89389-2406-47b...|2019|2021-04-02 00:05:02|  3|      v1|    v2|2021-04-02|\n",
      "+-------+-----+-------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Reading Spark table Parquet output\n",
      "+-------+-----+-------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|     id|month|     sk|txt|                uuid|year| modified_timestamp|  z|schema-v|data-v|  trade_dt|\n",
      "+-------+-----+-------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|4000011|    2|4000011|[P]|dbf28c6d-6f94-449...|2019|2021-04-02 00:05:02|  3|      v1|    v2|2021-04-02|\n",
      "|4000017|    1|4000017|[V]|96fa033f-0fb0-4d5...|2019|2021-04-02 00:05:02|  3|      v1|    v2|2021-04-02|\n",
      "|4000047|   10|4000047|[Z]|93c488d8-b882-442...|2019|2021-04-02 00:05:02|  3|      v1|    v2|2021-04-02|\n",
      "|4000051|    2|4000051|[D]|4464b11b-14aa-4ca...|2019|2021-04-02 00:05:02|  3|      v1|    v2|2021-04-02|\n",
      "|4000071|    4|4000071|[X]|bdb89389-2406-47b...|2019|2021-04-02 00:05:02|  3|      v1|    v2|2021-04-02|\n",
      "+-------+-----+-------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"Reading direct Parquet output\")\n",
    "spark.read.parquet(\"s3://vasveena-test-demo/temp6/parquet-perf/catalog/example_iceberg_perf_test_6/\").show(5)\n",
    "println(\"Reading Spark table Parquet output\")\n",
    "spark.sql(\"select * from iceberg_table_normaldf limit 5\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d73713",
   "metadata": {},
   "source": [
    "Now lets go ahead and list the S3 location where our DF was written in regular Parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af468f6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE z=0/\n",
      "                           PRE z=1/\n",
      "                           PRE z=2/\n",
      "                           PRE z=3/\n",
      "                           PRE z=4/\n",
      "                           PRE z=5/\n",
      "                           PRE z=6/\n",
      "                           PRE z=7/\n",
      "                           PRE z=8/\n",
      "                           PRE z=9/\n",
      "                           PRE z=a/\n",
      "                           PRE z=b/\n",
      "                           PRE z=c/\n",
      "                           PRE z=d/\n",
      "                           PRE z=e/\n",
      "                           PRE z=f/\n",
      "2021-05-06 03:45:28          0 _SUCCESS\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "aws s3 ls s3://vasveena-test-demo/temp6/parquet-perf/catalog/example_iceberg_perf_test_6/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8f1e29",
   "metadata": {},
   "source": [
    "Checking the S3 location where our DF was written in regular Spark table format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dd345568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE z=0/\n",
      "                           PRE z=1/\n",
      "                           PRE z=2/\n",
      "                           PRE z=3/\n",
      "                           PRE z=4/\n",
      "                           PRE z=5/\n",
      "                           PRE z=6/\n",
      "                           PRE z=7/\n",
      "                           PRE z=8/\n",
      "                           PRE z=9/\n",
      "                           PRE z=a/\n",
      "                           PRE z=b/\n",
      "                           PRE z=c/\n",
      "                           PRE z=d/\n",
      "                           PRE z=e/\n",
      "                           PRE z=f/\n",
      "2021-05-06 03:50:47          0 _SUCCESS\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "aws s3 ls s3://vasveena-test-demo/iceberg/tables/catalog/example_iceberg_perf_test_normaldf/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fad8355",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "As expected, the S3 partitions are not hashed in both Spark table format and direct Parquet writes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32df9b4",
   "metadata": {},
   "source": [
    "<a id=\"dftest\"></a>\n",
    "## Dataframe Read/Write tests with Iceberg format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00943c1d",
   "metadata": {},
   "source": [
    "### Bulk insert our input dataframe in Iceberg format\n",
    "\n",
    "Now lets try to write the transformed dataframe in Iceberg format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9f1a81f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f69971ae7f2b4053b86f42eb40d25b71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: Long = 642391499352314\n",
      "s3_location_idf: String = s3://vasveena-test-demo/iceberg/tables/catalog/example_iceberg_perf_test_dfonly2\n",
      "data_location_idf: String = s3://vasveena-test-demo/iceberg/tables/data/dfonly2/\n",
      "duration: String = 139.448695328seconds\n"
     ]
    }
   ],
   "source": [
    "val t1 = System.nanoTime\n",
    "\n",
    "val s3_location_idf=s\"s3://vasveena-test-demo/iceberg/tables/catalog/example_iceberg_perf_test_dfonly2\"\n",
    "val data_location_idf=s\"s3://vasveena-test-demo/iceberg/tables/data/dfonly2/\"\n",
    "\n",
    "//orderBy clause is required to avoid error \"java.lang.IllegalStateException: Already closed files for partition: z=6/schema_v=v1/data_v=v2/trade_dt=trade_dt\"\n",
    "(input_df2.orderBy(\"z\",\"`schema-v`\",\"`data-v`\",\"trade_dt\").write.mode(\"OVERWRITE\")\n",
    "    .option(\"path\", s3_location_idf)\n",
    "    .option(\"write.object-storage.enabled\",true)\n",
    "    .option(\"write.object-storage.path\",data_location_idf)\n",
    "    .partitionBy(\"z\",\"`schema-v`\",\"`data-v`\",\"trade_dt\").format(\"iceberg\")\n",
    "    .saveAsTable(\"iceberg_table_dfonly2\"))\n",
    "\n",
    "val duration = (System.nanoTime - t1) / 1e9d + \"seconds\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31ee690",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Reading output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d09dfcba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d94d31a7294edc8b0a71b4f8710f2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data in iceberg format\n",
      "+-------+-----+-------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|     id|month|     sk|txt|                uuid|year| modified_timestamp|  z|schema-v|data-v|  trade_dt|\n",
      "+-------+-----+-------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|6000003|    6|6000003|[J]|6ed8ea5f-1693-441...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|6000006|    7|6000006|[M]|d8c42b7a-168f-477...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|6000008|    2|6000008|[O]|dad5d203-faf0-413...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|6000046|    3|6000046|[A]|b8ec2d30-2ea8-414...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|6000052|   11|6000052|[G]|61cb5988-97c0-49e...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "+-------+-----+-------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Using SparkSQL\n",
      "+-------+-----+-------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|     id|month|     sk|txt|                uuid|year| modified_timestamp|  z|schema-v|data-v|  trade_dt|\n",
      "+-------+-----+-------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|9000006|    4|9000006|[C]|b5dcb9d9-d6cb-46b...|2019|2021-04-02 00:05:02|  1|      v1|    v2|2021-04-02|\n",
      "|9000015|    5|9000015|[L]|bbbc585d-86df-44d...|2019|2021-04-02 00:05:02|  1|      v1|    v2|2021-04-02|\n",
      "|9000039|   10|9000039|[J]|915bf8cd-a048-444...|2019|2021-04-02 00:05:02|  1|      v1|    v2|2021-04-02|\n",
      "|9000052|   12|9000052|[W]|c2a6311b-2c4f-43d...|2019|2021-04-02 00:05:02|  1|      v1|    v2|2021-04-02|\n",
      "|9000062|    1|9000062|[G]|4bae3802-ebf1-42e...|2019|2021-04-02 00:05:02|  1|      v1|    v2|2021-04-02|\n",
      "+-------+-----+-------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"Reading data in iceberg format\")\n",
    "spark.read.format(\"iceberg\").load(\"iceberg_table_dfonly2\").show(5)\n",
    "println(\"Using SparkSQL\")\n",
    "spark.sql(\"select * from iceberg_table_dfonly2 limit 5\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eda8a7a",
   "metadata": {},
   "source": [
    "Listing S3 location of Iceberg data and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "b049bddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE z=0/\n",
      "                           PRE z=1/\n",
      "                           PRE z=2/\n",
      "                           PRE z=3/\n",
      "                           PRE z=4/\n",
      "                           PRE z=5/\n",
      "                           PRE z=6/\n",
      "                           PRE z=7/\n",
      "                           PRE z=8/\n",
      "                           PRE z=9/\n",
      "                           PRE z=a/\n",
      "                           PRE z=b/\n",
      "                           PRE z=c/\n",
      "                           PRE z=d/\n",
      "                           PRE z=e/\n",
      "                           PRE z=f/\n",
      "2021-05-06 04:11:55       3604 00000-10a945d8-68ad-4336-92ce-cf5afdf77353.metadata.json\n",
      "2021-05-06 04:35:50       4524 00001-339f14f3-a0e0-4945-89ab-c545afa9f79d.metadata.json\n",
      "2021-05-06 04:11:54       9461 78b1612f-01f7-4f40-95ae-08cb525823a6-m0.avro\n",
      "2021-05-06 04:35:50       9454 b9200e4f-3018-4b14-9ade-d243869fa0f4-m0.avro\n",
      "2021-05-06 04:11:55       3566 snap-2533887051401835083-1-78b1612f-01f7-4f40-95ae-08cb525823a6.avro\n",
      "2021-05-06 04:35:50       3568 snap-3245032220820910222-1-b9200e4f-3018-4b14-9ade-d243869fa0f4.avro\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'\\naws s3 ls \"s3://vasveena-test-demo/iceberg/tables/catalog/example_iceberg_perf_test_dfonly/data/\"\\n#Partitions listed but it is not stored in a location that is pre-hashed\\n\\naws s3 ls \"s3://vasveena-test-demo/iceberg/tables/catalog/example_iceberg_perf_test_dfonly/metadata/\"\\n#Lists iceberg metadata\\n\\naws s3 ls \"s3://vasveena-test-demo/iceberg/tables/data/dfonly/\"\\n#This location is empty. Hence, throws error.\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-194-c6b7120a57bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\naws s3 ls \"s3://vasveena-test-demo/iceberg/tables/catalog/example_iceberg_perf_test_dfonly/data/\"\\n#Partitions listed but it is not stored in a location that is pre-hashed\\n\\naws s3 ls \"s3://vasveena-test-demo/iceberg/tables/catalog/example_iceberg_perf_test_dfonly/metadata/\"\\n#Lists iceberg metadata\\n\\naws s3 ls \"s3://vasveena-test-demo/iceberg/tables/data/dfonly/\"\\n#This location is empty. Hence, throws error.\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/notebook-env/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2379\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2380\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2381\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2382\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/notebook-env/lib/python3.7/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-103>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m/mnt/notebook-env/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/notebook-env/lib/python3.7/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'\\naws s3 ls \"s3://vasveena-test-demo/iceberg/tables/catalog/example_iceberg_perf_test_dfonly/data/\"\\n#Partitions listed but it is not stored in a location that is pre-hashed\\n\\naws s3 ls \"s3://vasveena-test-demo/iceberg/tables/catalog/example_iceberg_perf_test_dfonly/metadata/\"\\n#Lists iceberg metadata\\n\\naws s3 ls \"s3://vasveena-test-demo/iceberg/tables/data/dfonly/\"\\n#This location is empty. Hence, throws error.\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "aws s3 ls \"s3://vasveena-test-demo/iceberg/tables/catalog/example_iceberg_perf_test_dfonly/data/\"\n",
    "#Partitions listed but it is not stored in a location that is pre-hashed\n",
    "\n",
    "aws s3 ls \"s3://vasveena-test-demo/iceberg/tables/catalog/example_iceberg_perf_test_dfonly/metadata/\"\n",
    "#Lists iceberg metadata\n",
    "\n",
    "aws s3 ls \"s3://vasveena-test-demo/iceberg/tables/data/dfonly/\"\n",
    "#This location is empty. Hence, throws error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6d421c",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "S3 partitions are not hashed when using pure dataframe with iceberg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f1fb76",
   "metadata": {},
   "source": [
    "<a id=\"sparksqltest\"></a>\n",
    "## SparkSQL + Iceberg Read/Write test with Parquet Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1bc9a5",
   "metadata": {},
   "source": [
    "### Creating an output table in SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8f67e304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d845ec230369415985f833ef44eec489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- month: long (nullable = true)\n",
      " |-- sk: long (nullable = true)\n",
      " |-- txt: struct (nullable = true)\n",
      " |    |-- key1: string (nullable = true)\n",
      " |-- uuid: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- modified_timestamp: timestamp (nullable = true)\n",
      " |-- z: string (nullable = true)\n",
      " |-- schema-v: string (nullable = true)\n",
      " |-- data-v: string (nullable = true)\n",
      " |-- trade_dt: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"iceberg\").load(\"iceberg_table_dfonly2\").printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "16aa02bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1e7b788e50d40059e80c4fab0dfb1b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res191: org.apache.spark.sql.DataFrame = []\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE iceberg_table_sparksql2 (id bigint,\n",
    "                                       month bigint,\n",
    "                                       sk bigint,\n",
    "                                       txt struct<key1:string>,\n",
    "                                       uuid string,\n",
    "                                       year string,\n",
    "                                       modified_timestamp timestamp,\n",
    "                                       z string,\n",
    "                                       `schema_v` string,   --had to rename these columns due to error\n",
    "                                       `data_v` string,\n",
    "                                       trade_dt string)\n",
    "USING iceberg\n",
    "OPTIONS ( 'write.object-storage.enabled'=true,\n",
    "          'write.object-storage.path'='s3://vasveena-test-demo/iceberg/tables/data/sparksql2/')\n",
    "PARTITIONED BY (z,`schema_v`,`data_v`,trade_dt)\n",
    "LOCATION 's3://vasveena-test-demo/iceberg/tables/catalog/example_iceberg_perf_test_sparksql2'\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5220cc54",
   "metadata": {},
   "source": [
    "#### Gotcha\n",
    "\n",
    "During insert with input_df2, we will get an error \"Possibly unquoted identifier schema-v detected\" which happened even if those fields are quoted properly on both source and destination. So, changing col name from data-v, schema-v to data_v and schema_v on both source and destination tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "3b6205ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30390c3c812148b78146aeaecbd6e422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- month: long (nullable = true)\n",
      " |-- sk: long (nullable = true)\n",
      " |-- txt: struct (nullable = true)\n",
      " |    |-- key1: string (nullable = true)\n",
      " |-- uuid: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- modified_timestamp: timestamp (nullable = true)\n",
      " |-- z: string (nullable = true)\n",
      " |-- schema-v: string (nullable = false)\n",
      " |-- data-v: string (nullable = false)\n",
      " |-- trade_dt: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_df2.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "5dab484d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96089da1e4634a3baf50bbe46a330138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_df4: org.apache.spark.sql.DataFrame = [id: bigint, month: bigint ... 9 more fields]\n"
     ]
    }
   ],
   "source": [
    "val input_df4 = input_df2.withColumnRenamed(\"schema-v\", \"schema_v\").withColumnRenamed(\"data-v\", \"data_v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b00d0f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "887ee49ec279401ba13fd2f4658c1877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- month: long (nullable = true)\n",
      " |-- sk: long (nullable = true)\n",
      " |-- txt: struct (nullable = true)\n",
      " |    |-- key1: string (nullable = true)\n",
      " |-- uuid: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- modified_timestamp: timestamp (nullable = true)\n",
      " |-- z: string (nullable = true)\n",
      " |-- schema_v: string (nullable = false)\n",
      " |-- data_v: string (nullable = false)\n",
      " |-- trade_dt: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_df4.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "8ed29e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4101065c23f2443f9908dbd588d97117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: there was one deprecation warning (since 2.0.0); for details, enable `:setting -deprecation' or `:replay -deprecation'\n"
     ]
    }
   ],
   "source": [
    "input_df4.registerTempTable(\"inputdf4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd82e322",
   "metadata": {},
   "source": [
    "### Bulk insert data with iceberg format\n",
    "\n",
    "Now, lets do a bulk insert on iceberg table using pure SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "9dc07b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05a9baf500f146efbab389c9edd3765a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: Long = 650072184673361\n",
      "res210: org.apache.spark.sql.DataFrame = []\n",
      "duration: String = 140.217764581seconds\n"
     ]
    }
   ],
   "source": [
    "val t1 = System.nanoTime\n",
    "\n",
    "spark.sql(\"\"\"insert overwrite table iceberg_table_sparksql2\n",
    "             partition (z,`schema_v`,`data_v`,trade_dt) \n",
    "             select id, month, sk, txt, uuid,\n",
    "             year, modified_timestamp, z,\n",
    "             `schema_v`, `data_v`,'trade_dt'\n",
    "             from inputdf4\n",
    "             order by z,`schema_v`,`data_v`,trade_dt \"\"\") //order by clause is required to avoid error \"java.lang.IllegalStateException: Already closed files for partition: z=6/schema_v=v1/data_v=v2/trade_dt=trade_dt\"\n",
    "\n",
    "val duration = (System.nanoTime - t1) / 1e9d + \"seconds\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb58417",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Reading output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ddbc4654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec6597571eed4ea4ad0060ca636a5d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res187: org.apache.spark.sql.DataFrame = [id: bigint, month: bigint ... 9 more fields]\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select * from default.iceberg_table_sparksql\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1083764",
   "metadata": {},
   "source": [
    "Listing write object storage path. As we can see, the hashes are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "cbe93450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE 002fb1ef/\n",
      "                           PRE 00a4ce44/\n",
      "                           PRE 00b54463/\n",
      "                           PRE 00b9eb53/\n",
      "                           PRE 010c1ca4/\n",
      "                           PRE 0196f04c/\n",
      "                           PRE 01f2075d/\n",
      "                           PRE 026cc0ea/\n",
      "                           PRE 02c481ec/\n",
      "                           PRE 02e728f8/\n",
      "                           PRE 0328bcc6/\n",
      "                           PRE 03414b3b/\n",
      "                           PRE 0373fea9/\n",
      "                           PRE 03b68b4b/\n",
      "                           PRE 04ec9c8e/\n",
      "                           PRE 052673bc/\n",
      "                           PRE 05ffdbe9/\n",
      "                           PRE 061b80ad/\n",
      "                           PRE 064f3f29/\n",
      "                           PRE 0696b1cb/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "aws s3 ls \"s3://vasveena-test-demo/iceberg/tables/data/sparksql2/\" | head -n 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95e2e18",
   "metadata": {},
   "source": [
    "If we list one of the above hashes, we get the following:\n",
    "\n",
    "```\n",
    "aws s3 ls s3://vasveena-test-demo/iceberg/tables/data/sparksql2/002fb1ef/catalog/example_iceberg_perf_test_sparksql2/z=c/schema_v=v1/data_v=v2/\n",
    "2021-05-06 02:38:59          0 trade_dt=trade_dt_$folder$ \n",
    "```\n",
    "\n",
    "Similarly, we can list the S3 metadata location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "92248a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-06 06:32:57       2938 00000-de09a45e-f0ee-4bf0-b75c-0c4bb7937ee6.metadata.json\n",
      "2021-05-06 06:47:17       3965 00001-0d1176aa-13f0-45aa-9a63-acf219999cce.metadata.json\n",
      "2021-05-06 06:47:16       9527 9f9249a7-8907-4ee3-a1db-e8b0043d359c-m0.avro\n",
      "2021-05-06 06:47:17       3565 snap-8573294365950336448-1-9f9249a7-8907-4ee3-a1db-e8b0043d359c.avro\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "aws s3 ls \"s3://vasveena-test-demo/iceberg/tables/catalog/example_iceberg_perf_test_sparksql2/metadata/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d45cbc5",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "Spark + Iceberg integration with write object storage path works as documented while using pure SparkSQL. This is true for both Scala/PySpark implementations\n",
    "\n",
    "Table created via DF API and SparkSQL look identical based on diffchecker. Checked \"show create table\" from Hive catalog. This is something that needs to be checked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2aa555",
   "metadata": {},
   "source": [
    "<a id=\"hybrid\"></a>\n",
    "# Hybrid implementation with DF + DS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15ad673",
   "metadata": {},
   "source": [
    "Now that we have established pure SparkSQL works as expected, we will look into extending this implementation to DF + DS. The reason is, FINRA uses Datasets currently to read/write. So, trying to reduce changes as much as possible. \n",
    "\n",
    "FINRA's current workloads look like following\n",
    "\n",
    "&emsp;&emsp;&emsp;&emsp;1) Read BZ2/Parquet input files from S3 into dataset <br>\n",
    "&emsp;&emsp;&emsp;&emsp;2) Perform transformations on input dataset <br>\n",
    "&emsp;&emsp;&emsp;&emsp;3) Store intermediate output in HDFS<br>\n",
    "&emsp;&emsp;&emsp;&emsp;4) Do 2 and 3 for all N steps <br>\n",
    "&emsp;&emsp;&emsp;&emsp;5) Combine all HDFS output and write to S3 location (final destination)<br>\n",
    "\n",
    "Idea is to do something like following\n",
    "\n",
    "&emsp;&emsp;&emsp;&emsp;1) Maintain iceberg tables for input/output data in Hive catalog<br>\n",
    "&emsp;&emsp;&emsp;&emsp;2) Read input data from input iceberg table from S3 into a dataframe and cast to Dataset<br>\n",
    "&emsp;&emsp;&emsp;&emsp;3) Perform transformations on top of dataset <br>\n",
    "&emsp;&emsp;&emsp;&emsp;4) Store intermediate result in HDFS <br>\n",
    "&emsp;&emsp;&emsp;&emsp;5) Do 3 and 4 for all N steps<br>\n",
    "&emsp;&emsp;&emsp;&emsp;6) Combine all HDFS output and write to S3 location in Iceberg format (final destination)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f04d44",
   "metadata": {},
   "source": [
    "<a id=\"sparksqldftest\"></a>\n",
    "## Hybrid tests with Dataframes + SparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56979211",
   "metadata": {},
   "source": [
    "In our previous cases, the input was a Dataframe but the resultant table was created and inserted to via SparkSQL. Let us try an hybrid implementation where input is dataframe and the output table is created from SparkSQL. However, lets do the insert via DF operation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b573feb5",
   "metadata": {},
   "source": [
    "### Creating a new iceberg table \n",
    "\n",
    "There must be other ways to achieve this with non-SQL syntax (for type safety)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "0e49d6fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1835ffde400407c8910801d01f671ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res213: org.apache.spark.sql.DataFrame = []\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE iceberg_table_sparksqldf (id bigint,\n",
    "                                       month bigint,\n",
    "                                       sk bigint,\n",
    "                                       txt struct<key1:string>,\n",
    "                                       uuid string,\n",
    "                                       year string,\n",
    "                                       modified_timestamp timestamp,\n",
    "                                       z string,\n",
    "                                       `schema_v` string,   --had to rename these columns due to error\n",
    "                                       `data_v` string,\n",
    "                                       trade_dt string)\n",
    "USING iceberg\n",
    "OPTIONS ( 'write.object-storage.enabled'=true,\n",
    "          'write.object-storage.path'='s3://vasveena-test-demo/iceberg/tables/data/sparksqldf/')\n",
    "PARTITIONED BY (z,`schema_v`,`data_v`,trade_dt)\n",
    "LOCATION 's3://vasveena-test-demo/iceberg/tables/catalog/example_iceberg_perf_test_sparksqldf'\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd4ded9",
   "metadata": {},
   "source": [
    "### Bulk inserting data into the iceberg table\n",
    "\n",
    "This time lets insert using insertInto on input DF. \n",
    "\n",
    "#### Gotcha \n",
    "\n",
    "Partition fields of the input dataframe should be at end to match with the output iceberg table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "695d6f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a90f26757a714b7fae4e30a891e9997b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: Long = 652865379993309\n",
      "duration: String = 159.165016451seconds\n"
     ]
    }
   ],
   "source": [
    "val t1 = System.nanoTime\n",
    "\n",
    "//order by clause is needed to avoid error \"Caused by: java.lang.IllegalStateException: Already closed files for partition: z=c/schema_v=v1/data_v=v2/trade_dt=2021-04-02\"\n",
    "input_df4.orderBy(\"z\",\"`schema-v`\",\"`data-v`\",\"trade_dt\").write.mode(\"overwrite\").insertInto(\"iceberg_table_sparksqldf\")\n",
    "\n",
    "val duration = (System.nanoTime - t1) / 1e9d + \"seconds\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8bc14c",
   "metadata": {},
   "source": [
    "### Reading data from table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "dd98133d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51aa9c1e82c34c3bb0d28fd0dc3dea5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|      id|month|      sk|txt|                uuid|year| modified_timestamp|  z|schema_v|data_v|  trade_dt|\n",
      "+--------+-----+--------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|38000025|    3|38000025|[L]|60098a8c-0bf9-47f...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|38000031|    3|38000031|[R]|a4bf6610-36d0-43b...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|38000043|    6|38000043|[D]|495d0f6d-dc23-4e6...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|38000069|    6|38000069|[D]|9b85c17c-74bd-403...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|38000075|   12|38000075|[J]|64ab0d92-7084-4ca...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "+--------+-----+--------+---+--------------------+----+-------------------+---+--------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(\"iceberg_table_sparksqldf\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3ced4d",
   "metadata": {},
   "source": [
    "### Listing S3 locations\n",
    "\n",
    "Now lets check the S3 location of \"write.object-storage.enabled\". We can see hashing has been taken care of. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "d8896864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE 0061f845/\n",
      "                           PRE 00cb3cf0/\n",
      "                           PRE 00cd5e67/\n",
      "                           PRE 00eea961/\n",
      "                           PRE 00f6a166/\n",
      "                           PRE 0114e19d/\n",
      "                           PRE 0133e402/\n",
      "                           PRE 0134f2be/\n",
      "                           PRE 013f809b/\n",
      "                           PRE 01824c0d/\n",
      "                           PRE 01d76c28/\n",
      "                           PRE 0255b3ea/\n",
      "                           PRE 0293050d/\n",
      "                           PRE 02e7aef7/\n",
      "                           PRE 033d490e/\n",
      "                           PRE 040b36ee/\n",
      "                           PRE 040dca1c/\n",
      "                           PRE 041ac4ec/\n",
      "                           PRE 044466aa/\n",
      "                           PRE 0445d322/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "aws s3 ls \"s3://vasveena-test-demo/iceberg/tables/data/sparksqldf/\" | head -n 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08f7468",
   "metadata": {},
   "source": [
    "Listing metadata on S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "dca527c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-06 07:23:11       2940 00000-c852f0b7-822e-4c93-927b-c110fe88d7de.metadata.json\n",
      "2021-05-06 07:34:09       3969 00001-ef48b376-a213-4285-8bd4-1bf0038ad693.metadata.json\n",
      "2021-05-06 07:34:09       9519 9b661ca2-2703-4eef-8258-25f3c8d9eda5-m0.avro\n",
      "2021-05-06 07:34:09       3569 snap-1712768399067180034-1-9b661ca2-2703-4eef-8258-25f3c8d9eda5.avro\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "aws s3 ls \"s3://vasveena-test-demo/iceberg/tables/catalog/example_iceberg_perf_test_sparksqldf/metadata/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d61e61",
   "metadata": {},
   "source": [
    "<a id=\"sparksqldstest\"></a>\n",
    "## Hybrid tests with Dataset + SparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5482d377",
   "metadata": {},
   "source": [
    "Now lets try something similar with dataset. We will create a dataset object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba00ac8e",
   "metadata": {},
   "source": [
    "### Creating Dataset from Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "5a376037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1d11083a6c549d9a24a59e46b3a8609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.types._\n",
      "import org.apache.spark.sql.Dataset\n",
      "import org.apache.spark.sql.Row\n",
      "defined class TxtStruct\n",
      "defined class Random\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.Dataset\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "/* val schema = (new StructType()\n",
    "               .add(\"id\", IntegerType)\n",
    "               .add(\"month\",IntegerType)\n",
    "               .add(\"sk\",IntegerType)\n",
    "               .add(\"txt\",new StructType()\n",
    "                    .add(\"key1\",StringType))\n",
    "               .add(\"uuid\",StringType)\n",
    "               .add(\"year\",StringType)\n",
    "               .add(\"modified_timestamp\",TimestampType)\n",
    "               .add(\"z\",StringType)\n",
    "               .add(\"schema_v\", StringType)\n",
    "               .add(\"data_v\", StringType)\n",
    "               .add(\"trade_dt\", StringType)) */\n",
    "\n",
    "case class TxtStruct(key1: String)\n",
    "\n",
    "case class Random(id: BigInt, \n",
    "                  month: BigInt, \n",
    "                  sk: BigInt, \n",
    "                  txt: Seq[TxtStruct], \n",
    "                  uuid: String, \n",
    "                  year: String,\n",
    "                  modified_timestamp: java.sql.Timestamp, \n",
    "                  z: String, \n",
    "                  schema_v: String, \n",
    "                  data_v: String, \n",
    "                  trade_dt: String)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "c578718a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eba3a90f52bb49bc87a07fb7cc5978be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds_input: org.apache.spark.sql.Dataset[Random] = [id: bigint, month: bigint ... 9 more fields]\n"
     ]
    }
   ],
   "source": [
    "val ds_input = input_df4.select($\"id\",$\"month\",$\"sk\"\n",
    "                                //,$\"txt\".cast(\"array<struct<key1:string>>\")\n",
    "                                ,(array($\"txt\")) as \"txt\"\n",
    "                                ,$\"uuid\",$\"year\",$\"modified_timestamp\",$\"z\"\n",
    "                                ,$\"schema_v\",$\"data_v\",$\"trade_dt\").as[Random]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "4a9560a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "271f33196e45425593cabf6a3e792ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- month: long (nullable = true)\n",
      " |-- sk: long (nullable = true)\n",
      " |-- txt: array (nullable = false)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- key1: string (nullable = true)\n",
      " |-- uuid: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- modified_timestamp: timestamp (nullable = true)\n",
      " |-- z: string (nullable = true)\n",
      " |-- schema_v: string (nullable = false)\n",
      " |-- data_v: string (nullable = false)\n",
      " |-- trade_dt: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds_input.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bd15d8",
   "metadata": {},
   "source": [
    "### Creating a new iceberg table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "1c3ad32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "113b6519b92246d79459d1db5f008e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res384: org.apache.spark.sql.DataFrame = []\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE iceberg_table_sparksqlds6(id bigint,\n",
    "                                       month bigint,\n",
    "                                       sk bigint,\n",
    "                                       -- txt struct<key1:string>,\n",
    "                                       txt array<struct<key1:string>>,\n",
    "                                       uuid string,\n",
    "                                       year string,\n",
    "                                       modified_timestamp timestamp,\n",
    "                                       z string,\n",
    "                                       `schema_v` string,   --had to rename these columns due to error\n",
    "                                       `data_v` string,\n",
    "                                       trade_dt string)\n",
    "USING iceberg\n",
    "OPTIONS ( 'write.object-storage.enabled'=true,\n",
    "          'write.object-storage.path'='s3://vasveena-test-demo/iceberg/tables/data/sparksqlds6/')\n",
    "PARTITIONED BY (z,`schema_v`,`data_v`,trade_dt)\n",
    "LOCATION 's3://vasveena-test-demo/iceberg/tables/catalog/example_iceberg_perf_test_sparksqlds6'\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876ac74e",
   "metadata": {},
   "source": [
    "### Bulk insert dataset to iceberg table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "885c8d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c32300db054b378f61480360af6201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: Long = 659476745464997\n",
      "duration: String = 169.715551177seconds\n"
     ]
    }
   ],
   "source": [
    "val t1 = System.nanoTime\n",
    "\n",
    "//order by clause is needed to avoid error \"Caused by: java.lang.IllegalStateException: Already closed files for partition: z=c/schema_v=v1/data_v=v2/trade_dt=2021-04-02\"\n",
    "ds_input.orderBy(\"z\",\"`schema-v`\",\"`data-v`\",\"trade_dt\").write.mode(\"overwrite\").insertInto(\"iceberg_table_sparksqlds6\")\n",
    "\n",
    "val duration = (System.nanoTime - t1) / 1e9d + \"seconds\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66173b4a",
   "metadata": {},
   "source": [
    "### Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "bd08c072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8494e8ef7b6c40159796c3cc934a6d0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+-----+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|      id|month|      sk|  txt|                uuid|year| modified_timestamp|  z|schema_v|data_v|  trade_dt|\n",
      "+--------+-----+--------+-----+--------------------+----+-------------------+---+--------+------+----------+\n",
      "|38000025|    3|38000025|[[L]]|60098a8c-0bf9-47f...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|38000031|    3|38000031|[[R]]|a4bf6610-36d0-43b...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|38000043|    6|38000043|[[D]]|495d0f6d-dc23-4e6...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|38000069|    6|38000069|[[D]]|9b85c17c-74bd-403...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "|38000075|   12|38000075|[[J]]|64ab0d92-7084-4ca...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|\n",
      "+--------+-----+--------+-----+--------------------+----+-------------------+---+--------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(\"iceberg_table_sparksqlds6\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8166d2",
   "metadata": {},
   "source": [
    "### Listing S3 locations\n",
    "\n",
    "Now lets check the S3 location of \"write.object-storage.enabled\". We can see hashing has been taken care of. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "b4f42a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE 0333adfe/\n",
      "                           PRE 0a311088/\n",
      "                           PRE 11115d68/\n",
      "                           PRE 1c72097b/\n",
      "                           PRE 1fe6cac8/\n",
      "                           PRE 23c08f89/\n",
      "                           PRE 2910e802/\n",
      "                           PRE 2c6f894c/\n",
      "                           PRE 3fed2e9d/\n",
      "                           PRE 4564c4dc/\n",
      "                           PRE 4d18cde7/\n",
      "                           PRE 5fff9515/\n",
      "                           PRE 63fcfe26/\n",
      "                           PRE 751ff565/\n",
      "                           PRE 75d0d81d/\n",
      "                           PRE 7ad4c521/\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "aws s3 ls \"s3://vasveena-test-demo/iceberg/tables/data/sparksqlds6/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "e6b846cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d6236e011d4a97a335ba85f317ec85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res390: org.apache.spark.sql.Dataset[Random] = [id: bigint, month: bigint ... 9 more fields]\n"
     ]
    }
   ],
   "source": [
    "ds_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b80290",
   "metadata": {},
   "source": [
    "Listing metadata on S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "83651fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-06 09:20:46       3076 00000-66de4ce7-3ae1-4efb-b73d-e39cd4d65fc5.metadata.json\n",
      "2021-05-06 09:24:30       4107 00001-f8969a4d-52c2-4f37-a8a6-9c7072fdb5c8.metadata.json\n",
      "2021-05-06 09:24:30       9587 698b5c19-2324-47da-a0d4-3a207de7fb05-m0.avro\n",
      "2021-05-06 09:24:30       3565 snap-9006403180404379398-1-698b5c19-2324-47da-a0d4-3a207de7fb05.avro\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "aws s3 ls \"s3://vasveena-test-demo/iceberg/tables/catalog/example_iceberg_perf_test_sparksqlds6/metadata/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3305dea",
   "metadata": {},
   "source": [
    "<a id=\"udf\"></a>\n",
    "## Calling UDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14414d9e",
   "metadata": {},
   "source": [
    "Since we are able to read data from Iceberg table into Dataframe/Dataset, we can also manipulate data using UDF / Dataset APIs such as copy or flatMapGroups. These APIs are heavily leveraged by FINRA Catlinker. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af74bf60",
   "metadata": {},
   "source": [
    "### Dataset APIs\n",
    "\n",
    "Let us first try Dataset APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "83d0a09f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb4fdd00f16483e8cc0f7f986e7cdf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds: org.apache.spark.sql.Dataset[Random] = [id: bigint, month: bigint ... 9 more fields]\n"
     ]
    }
   ],
   "source": [
    "val ds = spark.table(\"iceberg_table_sparksqlds6\").as[Random]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "c0f149ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5b5374703b4c53a5c3b2996d8b120c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- month: long (nullable = true)\n",
      " |-- sk: long (nullable = true)\n",
      " |-- txt: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- key1: string (nullable = true)\n",
      " |-- uuid: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- modified_timestamp: timestamp (nullable = true)\n",
      " |-- z: string (nullable = true)\n",
      " |-- schema_v: string (nullable = true)\n",
      " |-- data_v: string (nullable = true)\n",
      " |-- trade_dt: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb89954",
   "metadata": {},
   "source": [
    "#### Creating a random function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "fffcf64a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a0c8512eee5481b9755c638ea11b6ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "somefunc: (trade_dt: String, values: Iterator[Random])TraversableOnce[(String, Random)]\n"
     ]
    }
   ],
   "source": [
    "def somefunc(trade_dt:String, values:Iterator[Random]): TraversableOnce[(String,Random)] = {\n",
    "  println(\"Print from somefunc\")\n",
    "  return values.map(x => (trade_dt, x))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e596a3",
   "metadata": {},
   "source": [
    "Use flatMapGroups to call somefunc over trade_dt groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "153dcea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff379ea9c0e8481489d822d29ce74fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "udf_ds: org.apache.spark.sql.Dataset[(String, Random)] = [_1: string, _2: struct<id: decimal(38,0), month: decimal(38,0) ... 9 more fields>]\n",
      "+----------+--------------------+\n",
      "|        _1|                  _2|\n",
      "+----------+--------------------+\n",
      "|2021-04-02|[43215124, 6, 432...|\n",
      "|2021-04-02|[32324242, 3, 323...|\n",
      "|2021-04-02|[43215133, 8, 432...|\n",
      "|2021-04-02|[32324257, 1, 323...|\n",
      "|2021-04-02|[43215141, 9, 432...|\n",
      "+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val udf_ds = (ds.groupByKey(t => t.trade_dt)\n",
    "              .flatMapGroups(somefunc))\n",
    "\n",
    "udf_ds.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cc7cbc",
   "metadata": {},
   "source": [
    "We can write this result back into an existing Iceberg table if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab009ccf",
   "metadata": {},
   "source": [
    "### Dataframe UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "ae509929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c5a6f1c3bb64ee4984178fc90e4508d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df: org.apache.spark.sql.DataFrame = [id: bigint, month: bigint ... 9 more fields]\n"
     ]
    }
   ],
   "source": [
    "val df = spark.table(\"iceberg_table_sparksqlds6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0886496",
   "metadata": {},
   "source": [
    "Creating a test UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "30f14461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "868e95b1bbdb42679d6f1021b9eab61c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "someudf: java.math.BigDecimal => String\n"
     ]
    }
   ],
   "source": [
    "def someudf = (sk:java.math.BigDecimal) => {\n",
    "  println(\"Print from UDF\")\n",
    "  ((sk+\"1\")).toString\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "ec3ced15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37b49f15e8e2405e89e315ec9f267fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.functions.udf\n",
      "someUDF: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($$$82b5b23cea489b2712a1db46c77e458$$$$w$$Lambda$6134/2001105709@dc9b50b,StringType,List(Some(class[value[0]: decimal(38,18)])),None,true,true)\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.udf\n",
    "val someUDF = udf(someudf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "0c52e5be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9f68702bacc4147a21fc17c6f5c9a17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "udf_df: org.apache.spark.sql.DataFrame = [id: bigint, month: bigint ... 10 more fields]\n",
      "+--------+-----+--------+-----+--------------------+----+-------------------+---+--------+------+----------+--------------------+\n",
      "|      id|month|      sk|  txt|                uuid|year| modified_timestamp|  z|schema_v|data_v|  trade_dt|              udfcol|\n",
      "+--------+-----+--------+-----+--------------------+----+-------------------+---+--------+------+----------+--------------------+\n",
      "|38000025|    3|38000025|[[L]]|60098a8c-0bf9-47f...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|38000025.00000000...|\n",
      "|38000031|    3|38000031|[[R]]|a4bf6610-36d0-43b...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|38000031.00000000...|\n",
      "|38000043|    6|38000043|[[D]]|495d0f6d-dc23-4e6...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|38000043.00000000...|\n",
      "|38000069|    6|38000069|[[D]]|9b85c17c-74bd-403...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|38000069.00000000...|\n",
      "|38000075|   12|38000075|[[J]]|64ab0d92-7084-4ca...|2019|2021-04-02 00:05:02|  0|      v1|    v2|2021-04-02|38000075.00000000...|\n",
      "+--------+-----+--------+-----+--------------------+----+-------------------+---+--------+------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val udf_df = df.withColumn(\"udfcol\",someUDF($\"id\"))\n",
    "udf_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "c614922c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6d7c43323947bdbb3f841b1dbd34ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res409: org.apache.spark.sql.DataFrame = []\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE iceberg_table_sparksqludf2(id bigint,\n",
    "                                       month bigint,\n",
    "                                       sk bigint,\n",
    "                                       -- txt struct<key1:string>,\n",
    "                                       txt array<struct<key1:string>>,\n",
    "                                       uuid string,\n",
    "                                       year string,\n",
    "                                       modified_timestamp timestamp,\n",
    "                                       udfcol string,\n",
    "                                       z string,\n",
    "                                       `schema_v` string,   --had to rename these columns due to error\n",
    "                                       `data_v` string,\n",
    "                                       trade_dt string)\n",
    "USING iceberg\n",
    "OPTIONS ( 'write.object-storage.enabled'=true,\n",
    "          'write.object-storage.path'='s3://vasveena-test-demo/iceberg/tables/data/sparksqludf2/')\n",
    "PARTITIONED BY (z,`schema_v`,`data_v`,trade_dt)\n",
    "LOCATION 's3://vasveena-test-demo/iceberg/tables/catalog/example_iceberg_perf_test_sparksqludf2'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "9529ce33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "288774bdb1914c24a8da0f5d823f9d21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Session 8 did not reach idle status in time. Current status is busy.\n"
     ]
    }
   ],
   "source": [
    "udf_df.orderBy(\"z\",\"`schema_v`\",\"`data_v`\",\"trade_dt\").write.mode(\"overwrite\").insertInto(\"iceberg_table_sparksqludf2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279f7ffe",
   "metadata": {},
   "source": [
    "### Listing S3 location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "8ab4e29f",
   "metadata": {},
   "outputs": [
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'\\naws s3 ls \"s3://vasveena-test-demo/iceberg/tables/data/sparksqludf2/\"\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-377-8a81b778194a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\naws s3 ls \"s3://vasveena-test-demo/iceberg/tables/data/sparksqludf2/\"\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/notebook-env/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2379\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2380\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2381\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2382\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/notebook-env/lib/python3.7/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-103>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m/mnt/notebook-env/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/notebook-env/lib/python3.7/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'\\naws s3 ls \"s3://vasveena-test-demo/iceberg/tables/data/sparksqludf2/\"\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "aws s3 ls \"s3://vasveena-test-demo/iceberg/tables/data/sparksqludf2/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69d4c34",
   "metadata": {},
   "source": [
    "Result is as expected. So we should be able to transform intermediate data as long as we write the output to an existing Iceberg table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1be37a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
